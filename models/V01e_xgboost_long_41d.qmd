---
title: "Xgboost Modeling Long 41 days. Regression with percentile returns. V01e"
format: 
  html: 
    toc: true
    toc-depth: 2
    toc-location: left
    theme: darkly
    fontsize: 1.1em
    grid: 
      sidebar-width: 0px
      body-width: 2000px
      margin-width: 0px
---

# Intro 

* This is a regression model (based on a percentile of returns).  
* Main change from V01a and V01c is that we classified the forward returns differently.  Forward returns have been ordered in percentile and we put this quintile back into thirdile in the following way: bottom 1 and 2 back to #1, middle 3 to #2, top 4 and 5 quintile to #3 thirdile.  Bascially bottom 40% of returns are thirdile 1, middle 20% (the ones that are around 0%) in thirdile 2 and top 40% of returns are in thirdile 3. 
* The model engine is Xgboost  
* To predict: Forward return at 41 days and 61 days.  

Other idiosyncrasies of this model: 

* all returns are log returns

# Pre-processing 

## Setting up 

```{r}
#| label: setup
#| message: false
#| warning: false
#| eval: true

library(dplyr)
library(purrr)
library(readr)
library(lubridate)
library(tidyr)
library(glue)

the_path <- here::here()

ticker <- 'CVX'
num_days <- 41

model_type = '8wRegPerc'
yop <- glue('xgboost_v01e_', model_type, "_", ticker)
```

## Buidling df 

```{r}
#| label: building-df
#| message: false
#| warning: false

library(timetk)

sd_roll_31d <- slidify(.f = sd, .align = 'right', .period = 31)
mean_roll_3M <- slidify(.f = mean, .align = 'right', .period = 61)
sd_roll_3M <- slidify(.f = sd, .align = 'right', .period = 61)
sd_roll_63d <- slidify(.f = sd, .align = 'right', .period = 63)
mean_roll_7M <- slidify(.f = mean, .align = 'right', .period = 147)
sd_roll_7M <- slidify(.f = sd, .align = 'right', .period = 147)
mean_roll_1Y <- slidify(.f = mean, .align = 'right', .period = 251)
sd_roll_1Y <- slidify(.f = sd, .align = 'right', .period = 251)
corr_roll_199d <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 199)
corr_roll_1Y <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 251)

df <- read_csv(glue(the_path, '/data_stock_fmpr/', ticker, '.csv')) |> 
  arrange(date) |> 
  select(date, open, high, low, close, volume) |>
  mutate(ret_1d = log(close / lag(close, n = 1)), 
         ret_5d = log(close / lag(close, n = 5)), 
         ret_21d = log(close / lag(close, 21)), 
         
         roll_sd_ret1d_63days = sd_roll_63d(ret_1d), 
         roll_mean_ret1d_3M = mean_roll_3M(ret_1d), 
         roll_sd_ret1d_3M = sd_roll_3M(ret_1d), 
         above_sd_ret1d_3M = (ret_1d - roll_mean_ret1d_3M) / roll_sd_ret1d_3M, 
         roll_mean_ret5d_3M = mean_roll_3M(ret_5d), 
         roll_sd_ret5d_3M = sd_roll_3M(ret_5d), 
         above_sd_ret5d_3M = (ret_5d - roll_mean_ret5d_3M) / roll_sd_ret5d_3M, 
         roll_mean_ret5d_7M = mean_roll_7M(ret_5d), 
         roll_sd_ret5d_7M = sd_roll_7M(ret_5d), 
         above_sd_ret5d_7M = (ret_5d - roll_mean_ret5d_7M) / roll_sd_ret5d_7M, 
         roll_mean_ret21d_1Y = mean_roll_1Y(ret_21d), 
         roll_sd_ret21d_1Y = sd_roll_1Y(ret_21d),          
         above_sd_ret21d_1Y = (ret_21d - roll_mean_ret21d_1Y) / roll_sd_ret21d_1Y,
         
         intraday = (high - low) / open, 
         roll_mean_intraday_1Y = mean_roll_1Y(intraday), 
         roll_sd_intraday_1Y = sd_roll_1Y(intraday), 
         roll_sd_intraday_3M = sd_roll_3M(intraday), 
         above_sd_intraday_1Y = (intraday - roll_mean_intraday_1Y) / roll_sd_intraday_1Y, 
         
         ## old parameters
         ema20 = TTR::EMA(close, 20), 
         sma50 = TTR::SMA(close, 50), 
         sma100 = TTR::SMA(close, 100), 
         sma200 = TTR::SMA(close, 200),
         volum_sma200 = TTR::SMA(volume,  n = 200), 
         volum200_perc = log(volume / volum_sma200), 
         
         ma_cross_l = sma50 / sma100, 
         corr_ema20_sma50_199d = corr_roll_199d(ema20, sma50), 
         corr_ema20_sma50_1Y = corr_roll_1Y(ema20, sma50), 
         corr_sma50_sma200_199d = corr_roll_199d(sma50, sma200), 
         corr_sma50_sma200_1Y = corr_roll_1Y(sma50, sma200), 
         roll_sd_volu200_31days = sd_roll_31d(volum200_perc), 
         roll_sd_volu200_251days = sd_roll_1Y(volum200_perc), 
         forw_ret = log(lead(close, n = num_days) / close), 
         ord_class = ntile(forw_ret, 100))

df2 <- df |> 
  select(date, roll_sd_ret1d_63days, roll_sd_ret5d_3M, roll_sd_ret5d_7M, 
         above_sd_ret1d_3M, above_sd_ret5d_3M, above_sd_ret5d_7M, above_sd_ret21d_1Y,
         roll_sd_ret21d_1Y, above_sd_intraday_1Y, roll_sd_intraday_3M, 
         ma_cross_l, corr_ema20_sma50_199d, corr_ema20_sma50_1Y, 
         corr_sma50_sma200_199d, corr_sma50_sma200_1Y,  
         roll_sd_volu200_31days, roll_sd_volu200_251days, 
         ord_class) |> 
  drop_na()

df_model <- df2 |> filter(date < '2023-06-30')
```

## Resampling and Recipes 

```{r}
#| label: preprocessing
#| message: false
#| warning: false

############# SAMPLING FOR CROSS-VALIDATION ##########################
library(rsample)
min_size_training <- 500
number_of_samples <- 50
skippy <- 63
laggy <- 21
# get the size of the df we are using. 
validation_size <- floor(nrow(df_model) * 0.2 / number_of_samples) + 1
training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49

if (training_size < 500) {
  training_size <- min_size_training
  skippy <- floor((nrow(df_model) - validation_size - laggy - min_size_training) / number_of_samples) 
  training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49
}

time_slices <- rolling_origin(df_model, 
                              initial = training_size, assess = validation_size + laggy, 
                              lag = -laggy, skip = skippy, 
                              cumulative = FALSE)

######################################################################
library(recipes)
recipe_base <- recipe(formula = ord_class ~., data = df_model) |> 
  update_role(date, new_role="ID") 
```


# Modeling 

```{r}
#| label: modeling
#| message: false
#| warning: false

library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
model_xgboost <- boost_tree(trees = tune(), mtry = tune(), loss_reduction = tune(), 
                            tree_depth = tune(), 
                            learn_rate = tune(), min_n = tune()) %>% 
  set_engine("xgboost") |>  
  set_mode("regression") 

stopping_grid <-grid_latin_hypercube(trees(range = c(400, 800)), 
                                     loss_reduction(range = c(-10, 1.5)), 
                                     mtry(range = c(13L, 17L)), 
                                     learn_rate(range = c(-4, -2)), 
                                     min_n(range = c(17L, 31L)),   
                                     tree_depth(range = c(5L, 11L)), 
                                     size = 25)

wf <- workflow() |> add_recipe(recipe_base) |> add_model(model_xgboost)  

doParallel::registerDoParallel()
xgboost_resampling <- tune_grid(wf, time_slices, 
                                grid = stopping_grid, 
                                metrics = metric_set(rmse), 
                                control = control_resamples(save_pred = TRUE, 
                                                            save_workflow = TRUE))


```



# Variables importance & Metrics 

```{r}
######################################################################
############# ESTABLISHING METRICS  ##################################
######################################################################
yo_metric <- xgboost_resampling |> 
  collect_metrics() |> 
  arrange(mean)

write_csv(yo_metric, glue(the_path, "/models/model_metrics/", yop, ".csv"))
######################################################################

######################################################################


best_param <- yo_metric[1, ]
best_param

final_model <- finalize_model(model_xgboost, best_param)
final_model

final_wf <- workflow() |> add_recipe(recipe_base) |> add_model(final_model)
final_wf

model_base <- fit(object = final_wf, data = df_model)

write_rds(model_base, glue(the_path, "/models/model_raw/", yop, ".rda"))

library(vip)
df_prep <- prep(recipe_base)
var_importance <- final_model %>% 
  set_engine("xgboost", importance = "permutation") %>% 
  fit(ord_class ~ ., data = juice(df_prep) %>% select(-date)) %>%
  vi()

write_csv(var_importance, 
          glue(the_path, "/models/model_vars_imp/", yop, ".csv"))
```


# rsample

```{r}
library(lubridate)
ts_df <- time_slices |> 
  mutate(train = map(splits, training), 
         validation = map(splits, testing)) 

get_start_date <- function(x){ min(x$date) }
get_end_date <- function(x){ max(x$date) }

ts_df_dates <- ts_df |> 
  mutate(start_train_date = as_date(map_dbl(train, get_start_date)), 
         end_train_date = as_date(map_dbl(train, get_end_date)), 
         start_test_date = as_date(map_dbl(validation, get_start_date)), 
         end_test_date = as_date(map_dbl(validation, get_end_date))) |> 
  select(-splits, -train, -validation)

glimpse(ts_df_dates)

```
