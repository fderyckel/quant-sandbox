---
title: "Xgboost Modeling Long. Classification with thirdile. V3a"
format: 
  html: 
    toc: true
    toc-depth: 2
    toc-location: left
    theme: darkly
    fontsize: 1.1em
    grid: 
      sidebar-width: 0px
      body-width: 2000px
      margin-width: 0px
---

TODO



# Intro 

This model built on V1 and V2.  It removes the 5 least important variables and add a bunch of new ones. 


```{r}
#| label: setup
#| message: false
#| warning: false
#| eval: true

library(dplyr)
library(purrr)
library(readr)
library(lubridate)
library(tidyr)
library(glue)

the_path <- here::here()

ticker <- 'CVX'
num_days <- '83d'

yop <- glue('xgboost_v03a_short_', num_days, "_", ticker)

primes <- c(5, 11, 17, 19, 23, 29, 37, 47, 59, 71, 83, 97, 
            109, 127, 157, 179, 199, 223, 251)

conform_data <- function(ticker) {
  df <- read_csv(paste0(the_path, "/data_stock_fmpr/", ticker, ".csv"), 
                 show_col_types = FALSE) |> 
    rename(adjusted = adjClose) |> arrange(date)
}

get_returns_sd <- function(df) {
  for (i in primes) { 
    ret_name = glue('ret_', i, 'd')
    sd_name = glue('sd_', i, 'd') 
    roll_sd <- slidify(.f = sd, .period = i, .align = 'right') 
    df <- df |> mutate(!!as.name(ret_name) := log(adjusted / lag(adjusted, i)), 
                       !!as.name(sd_name) := roll_sd(ret_1d)) 
  }
  return(df)
}

get_corr <- function(df) {
  for (i in primes) { 
    corr_name = glue('corr_', i, 'd')
    roll_corr <- slidify(.f = ~cor(.x, .y), .period = i, .align = 'right') 
    df <- df |> mutate(!!as.name(corr_name) := roll_corr(adj_tick, adj_etf))
  }
  return(df)
}


create_relative_prices <- function(df_tick, df_etf) {
  df_tick <- conform_data(df_tick) |> select(date, adj_tick = adjusted)
  df_etf <- conform_data(df_etf) |> select(date, adj_etf = adjusted)
  df <- left_join(df_tick, df_etf, by = c('date')) |> 
    mutate(adjusted = adj_tick / adj_etf, 
           ret_1d = log(adjusted / lag(adjusted))) |> 
    drop_na()
  return(df)
}


```


```{r}
#| label: building-df
#| message: false
#| warning: false

library(timetk)

mean_roll_251d <- slidify(.f = mean, .align = 'right', .period = 251)
sd_roll_21d <- slidify(.f = sd, .align = 'right', .period = 21)
sd_roll_61d <- slidify(.f = sd, .align = 'right', .period = 61)
sd_roll_67d <- slidify(.f = sd, .align = 'right', .period = 67)
sd_roll_127d <- slidify(.f = sd, .align = 'right', .period = 127)
sd_roll_147d <- slidify(.f = sd, .align = 'right', .period = 147)
sd_roll_199d <- slidify(.f = sd, .align = 'right', .period = 199)
sd_roll_251d <- slidify(.f = sd, .align = 'right', .period = 251)
corr_roll_101d <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 101)
corr_roll_147d <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 147)
corr_roll_199d <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 199)
corr_roll_251d <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 251)


df <- read_csv(glue(the_path, '/data_stock_fmpr/', ticker, '.csv')) |> 
  arrange(date) |> 
  select(date, open, high, low, close, volume) |> 
  mutate(ret_1d = log(close / lag(close, n = 1)), 
         ret_5d = log(close / lag(close, n = 5)), 
         ret_21d = log(close / lag(close, 21)), 
         ret_71d = log(close / lag(close, 71)), 
         intraday = (high - low) / open, 
         roll_sd_intraday_61d = sd_roll_61d(intraday), 
         roll_sd_ret5d_61d = sd_roll_61d(ret_5d), 
         roll_sd_ret1d_67d = sd_roll_67d(ret_1d),
         roll_sd_ret1d_127d = sd_roll_127d(ret_1d),
         roll_sd_ret1d_147d = sd_roll_147d(ret_1d), 
         roll_sd_ret5d_147d = sd_roll_147d(ret_5d), 
         roll_sd_ret1d_199d = sd_roll_199d(ret_1d),
         too_corr1 = corr_roll_147d(roll_sd_ret1d_147d, roll_sd_ret5d_147d), 
         roll_sd_intraday_251d = sd_roll_251d(intraday),
         roll_sd_ret1d_251d = sd_roll_251d(ret_1d), 
         too_corr2 = corr_roll_251d(roll_sd_intraday_251d, roll_sd_ret1d_251d), 
         roll_sd_ret21d_251d = sd_roll_251d(ret_21d),  
         too_corr3 = corr_roll_199d(roll_sd_ret21d_251d, roll_sd_intraday_251d), 
         too_corr4 = corr_roll_199d(roll_sd_ret1d_127d, roll_sd_ret1d_199d),
         
         ema20 = TTR::EMA(close, 20), 
         sma50 = TTR::SMA(close, 50), 
         sma101 = TTR::SMA(close, 101),
         sma200 = TTR::SMA(close, 199),
         roll_mean_sma200_251d = mean_roll_251d(sma200), 
         roll_sd_sma200_251d = sd_roll_251d(sma200), 
         ma_cross_l = sma50 / sma101, 
         above_sd_sma200_251d = (sma200 - roll_mean_sma200_251d) / roll_sd_sma200_251d,
         corr_ema20_sma50_199d = corr_roll_199d(ema20, sma50), 
         corr_ema20_sma50_251d = corr_roll_251d(ema20, sma50), 
         corr_sma50_sma200_101d = corr_roll_101d(sma50, sma200), 
         corr_sma50_sma200_199d = corr_roll_199d(sma50, sma200), 
         corr_sma50_sma200_251d = corr_roll_251d(sma50, sma200), 
         
         volum_sma50 = TTR::SMA(volume,  n = 53),
         volum53_perc = log(volume / volum_sma50), 
         roll_sd_volu50_21d = sd_roll_21d(volum53_perc), 
         volum_sma100 = TTR::SMA(volume,  n = 101),
         volum100_perc = log(volume / volum_sma100), 
         roll_sd_volu100_61d = sd_roll_61d(volum100_perc), 
         volum_sma200 = TTR::SMA(volume,  n = 199), 
         volum200_perc = log(volume / volum_sma200), 
         roll_sd_volu200_251d = sd_roll_251d(volum200_perc), 
         
         forw_ret = log(lead(close, n = parse_number(num_days)) / close), 
         ord_class = as.factor(ntile(forw_ret, 3))) |> 
  select(date, ord_class, ret_71d, 
         ma_cross_l, above_sd_sma200_251d, 
         roll_sd_intraday_61d, roll_sd_ret5d_61d, 
         roll_sd_ret1d_147d, too_corr1, too_corr4, 
         too_corr3, too_corr2, roll_sd_ret21d_251d, 
         corr_sma50_sma200_101d, 
         corr_ema20_sma50_199d, corr_ema20_sma50_251d, 
         corr_sma50_sma200_199d, corr_sma50_sma200_251d, 
         roll_sd_volu50_21d, roll_sd_volu100_61d, roll_sd_volu200_251d
         ) 

tick1 <- 'DIA'
tick2 <- 'QQQ'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> get_returns_sd() |> 
  drop_na() |> 
  select(date, corr_127d, corr_251d, sd_157d, corr_109d, ret_127d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)

df_model <- left_join(df, df_rel, by = join_by(date))

tick1 <- 'XLE'
tick2 <- 'SPY'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> get_returns_sd() |> 
  drop_na() |> 
  select(date, corr_251d, corr_157d, corr_37d, sd_199d, sd_251d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)
df_model <- left_join(df_model, df_rel, by = join_by(date))

tick1 <- 'CVX'
tick2 <- 'XLE'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> get_returns_sd() |> 
  drop_na() |> 
  select(date, corr_199d, corr_97d, corr_83d, sd_223d, sd_251d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)
df_model <- left_join(df_model, df_rel, by = join_by(date))

tick1 <- 'OIH'
tick2 <- 'VDE'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> get_returns_sd() |> 
  drop_na() |> 
  select(date, corr_251d, corr_179d, corr_109d, sd_251d, sd_157d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)
df_model <- left_join(df_model, df_rel, by = join_by(date))

tick1 <- 'CVX'
tick2 <- 'UUP'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> get_returns_sd() |> 
  drop_na() |> 
  select(date, corr_199d, corr_157d, sd_223d, sd_127d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)
df_model <- left_join(df_model, df_rel, by = join_by(date))

df_model <- df_model 
df_new_xgb3a <- df_model |> filter(date > '2023-06-30')
df_model <- df_model |> filter(date < '2023-06-30') |> drop_na() 
```

# Model preparation 

```{r}
#| label: model_prep
#| message: false
#| warning: false

############# SAMPLING FOR CROSS-VALIDATION ##########################
library(rsample)
# create the CV validation resamples 
#time_slices = rolling_origin(df, initial = 1250, assess = 43, 
#                             lag = -20, skip = 83, cumulative = F)

min_size_training <- 500
number_of_samples <- 50
skippy <- 55
laggy <- 21
# get the size of the df we are using. 
validation_size <- floor(nrow(df_model) * 0.25 / number_of_samples) + 1
training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49

if (training_size < 500) {
  training_size <- min_size_training
  skippy <- floor((nrow(df_model) - validation_size - laggy - min_size_training) / number_of_samples) 
  training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49
}

time_slices <- rolling_origin(df_model, 
                              initial = training_size, assess = validation_size + laggy, 
                              lag = -laggy, skip = skippy, 
                              cumulative = FALSE)

############# Create the preprocessing steps ##########################
library(recipes)
recipe_base <- recipe(formula = ord_class ~., data = df_model) |> 
  update_role(date, new_role="ID") |> 
  step_YeoJohnson(all_numeric_predictors())

############# SAMPLING FOR CROSS-VALIDATION ##########################
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
model_xgboost <- boost_tree(trees = tune(), mtry = tune(), loss_reduction = tune(), 
                            tree_depth = tune(), 
                            learn_rate = tune(), min_n = tune()) %>% 
  set_engine("xgboost") |>  
  set_mode("classification") 

wf_xgboost <- workflow() |> add_recipe(recipe_base) |> add_model(model_xgboost)  

library(dials)
stopping_grid <- grid_latin_hypercube(trees(range = c(600, 1000)), 
                                      loss_reduction(range = c(-8, -1)), 
                                      mtry(range = c(12L, 19L)), 
                                      learn_rate(range = c(-4, -2)), 
                                      min_n(range = c(17L, 31L)),   
                                      tree_depth(range = c(11L, 17L)), 
                                      size = 29)

############# SAMPLING FOR CROSS-VALIDATION ##########################

rm(df, df_rel, laggy, min_size_training, primes, number_of_samples, tick1, tick2)
rm(skippy, conform_data, create_relative_prices, get_returns_sd, mean_roll_251d)
rm(corr_roll_101d, corr_roll_147d, corr_roll_199d, corr_roll_251d, get_corr)
rm(sd_roll_127d, sd_roll_147d, sd_roll_199d, sd_roll_21d, sd_roll_61d, 
   sd_roll_251d, sd_roll_67d)

```

# Running models and collecting metrics 

```{r}
library(doParallel)
registerDoParallel()
xgboost_xv_resamples <- tune_grid(wf_xgboost, resamples = time_slices, 
                            grid = stopping_grid, 
                            metrics = metric_set(accuracy, roc_auc),
                            control = control_resamples(verbose = T, save_pred = T))

metrics_xgboost <- xgboost_xv_resamples |> collect_metrics() |> 
  arrange(.metric, desc(mean))

df_pred <- xgboost_xv_resamples |> collect_predictions()
yo_soft_acc <- df_pred |> 
  mutate(soft_acc = case_when(.pred_class=="3" & ord_class == "3" ~ 1, 
                              .pred_class=="2" & ord_class == "2" ~ 1, 
                              .pred_class=="1" & ord_class == "1" ~ 1, 
                              .pred_class=="2" & ord_class == "3" ~ 0.5, 
                              .pred_class=="1" & ord_class == "2" ~ 0.5, 
                              TRUE ~ 0)) |> 
  group_by(.config) |> 
  summarize(mean_soft_acc = mean(soft_acc), 
            mtry = mean(mtry), tree_depth = mean(tree_depth), 
            trees = mean(trees), 
            min_n = mean(min_n), learn_rate = mean(learn_rate))

# if I predict a 3, is it indeed a 3? Higher is better
yo_precision <- df_pred |> 
  mutate(spec_3 = if_else(.pred_class == "1" & ord_class == "1", 1, 0)) |> 
  filter(.pred_class == "1") |> 
  group_by(.config) |> 
  summarize(mean_preci = sum(spec_3)/n())

# if there is a "3" in the actual returns, do I manage to predict it! Higher is better
yo_recall <- df_pred  |>  
  mutate(sens_3 = if_else(.pred_class == "1" & ord_class == "1", 1, 0)) |> 
  select(-mtry, -min_n, -learn_rate, -loss_reduction, -tree_depth) |> 
  filter(ord_class == "1") |> 
  group_by(.config) |> 
  summarize(mean_recall = sum(sens_3)/n())

# if I predict a 3, is it indeed a 3? Higher is better
yo_soft_preci <- df_pred |> 
  mutate(spec_3 = case_when(.pred_class=="1" & ord_class == "1" ~ 1, 
                            .pred_class=="1" & ord_class == "2" ~ 0.5, 
                            .pred_class=="1" & ord_class == "3" ~ 0, 
                            TRUE ~ 0)) |> 
  filter(.pred_class == "1") |>  
  group_by(.config) |> 
  summarize(mean_soft_preci = sum(spec_3)/n())

yo_cost <- left_join(yo_precision, yo_recall) |> left_join(yo_soft_preci) |> 
  left_join(yo_soft_acc) |>  
  mutate(soft_acc_rank = percent_rank(mean_soft_acc), 
         recall_rank = percent_rank(mean_recall), 
         preci_rank = percent_rank(mean_preci), 
         soft_preci_rank = percent_rank(mean_soft_preci), 
         avg_rank =  (0.25 * preci_rank) + (0.3 * soft_preci_rank) + 
           (0.15 * soft_acc_rank) + (0.3 * recall_rank))

yo_metric <- left_join(yo_precision, yo_soft_acc) |> left_join(yo_recall) |> 
  left_join(yo_soft_preci) |> left_join(yo_cost) |> 
  select(.config, avg_rank, mean_preci, mean_soft_preci, mean_recall, mean_soft_acc, 
         mtry, tree_depth, trees, min_n, learn_rate) |> 
  arrange(desc(avg_rank), mean_preci)

#rm(yo_sens, yo_spec, yo_cost3, yo_acc, yo_log, yo_cost_acc)

write_csv(yo_metric, glue(the_path, "/models/model_metrics/", yop, ".csv"))
```


```{r}
best_model <- yo_metric$.config[1]

best_param <- df_pred |> 
  filter(.config == best_model) |> slice_tail()
best_param

final_model <- finalize_model(model_xgboost, best_param)
final_model

final_wf <- workflow() |> add_recipe(recipe_base) |> add_model(final_model)
final_wf

model_base <- fit(object = final_wf, data = df_model)

write_rds(model_base, glue(the_path, "/models/model_raw/", yop, ".rda"))

library(vip)
df_prep <- prep(recipe_base)
var_importance <- final_model %>% 
  set_engine("xgboost", importance = TRUE) %>% 
  fit(ord_class ~ ., data = juice(df_prep) %>% select(-date)) %>%
  vi()

write_csv(var_importance, 
          glue(the_path, "/models/model_vars_imp/", yop, ".csv"))
```


# fitting model on new observations 

```{r}
xgboost_final_fit <- read_rds('model_raw/xgboost_v03a_CVX_61d_CVX.rda')
xgboost_pred <- predict(xgboost_final_fit, new_data = df_new_xgb3a)
xgboost_pred_prob <- predict(xgboost_final_fit, new_data = df_new_xgb3a, type = 'prob')
df_pred_xgboost <- bind_cols(df_new_xgb3a |> select(date, ord_class), 
                                 xgboost_pred, xgboost_pred_prob)


xgboost_final_fit2 <- read_rds('model_raw/xgboost_v03a_short_CVX_61d_CVX.rda')
xgboost_pred2 <- predict(xgboost_final_fit2, new_data = df_new_xgb3a)
xgboost_pred_prob2 <- predict(xgboost_final_fit2, new_data = df_new_xgb3a, type = 'prob')
df_pred_xgboost2 <- bind_cols(df_new_xgb3a |> select(date, ord_class), 
                                 xgboost_pred2, xgboost_pred_prob2) |> 
  rename(ord_class_29d = ord_class)

yo <- left_join(df_pred_xgboost, df_pred_xgboost2, by = join_by(date))
```



# Analyses of correlation 

```{r correlation}
#| message: false
#| warning: false

library(corrr)

df_cor <- df_model %>% select(-date, -ord_class) 
yo <- correlate(df_cor) %>% shave() %>% stretch()

df1 <- yo %>% filter(r >= 0.5 | r <= -0.5) |> 
  arrange(desc(abs(r)))

library(gt)

df1 |> gt()
```



# rsample

```{r}
library(lubridate)
ts_df <- time_slices |> 
  mutate(train = map(splits, training), 
         validation = map(splits, testing)) 

get_start_date <- function(x){ min(x$date) }
get_end_date <- function(x){ max(x$date) }

ts_df_dates <- ts_df |> 
  mutate(start_train_date = as_date(map_dbl(train, get_start_date)), 
         end_train_date = as_date(map_dbl(train, get_end_date)), 
         start_test_date = as_date(map_dbl(validation, get_start_date)), 
         end_test_date = as_date(map_dbl(validation, get_end_date))) |> 
  select(-splits, -train, -validation)

glimpse(ts_df_dates)

```

