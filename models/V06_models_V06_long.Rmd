---
title: "Complex Models V.6.2"
author: "FdR"
date: "12/4/2021"
output: html_document
---

# Focus: 
focus on 5 weeks.  This is a long only models.  
We have decided to make it a classification task using thirdile (aka predict a 1 or 2 or 3 based on bottom 1/3 of returns, middle 1/3 or top 1/3 of returns). 
Goals is to predict if the stock will be in the top 1/3-ile of returns in 25 trading days.  

Resampling is based on: 
* 13 time-slices, 40 days of prediction for each time-slices. That is each models will be evaluated on 520 predictions. That being said, we have left a 13 days between last day of training and first day of prediction.  
* we worked on 120 xgboost models with different hyperparameters.  

Metrics were calculated as an average of ranking score based on: 
* 15% ordered accuracy / soft-accuracy.  If totally random, expectation is 0.5.  So we want an ordered expectation > 0.5.  We have 1 if class and prediction are 3, 0.5 if prediction is 3 and class is 2, 0 if prediction is 3 and class is 1 (that's what we want to avoid).  We do the same with prediction of 2 and 1.  Always penalizing more wrong prediction of higher returns.
* 15% recall.  We want to be able to detect the 3 (but not at the expense of poor prediction). Random score should be 0.333.   We filter to only consider truth of 3. 
* 50% precision.  We want to make sure that when we predict a 3, it is indeed a 3.  So we give a 1 if predicted = truth = 3, otherwise a 0. So expected (random score) is 0.333.  We filter to only consider prediction of 3. 
* 20% soft_precision.  We mean by this that if we do not predict correctly a 3, it is still better for truth to be 2 iinstead of a class of 1. We still want to keep an order, so we built it so that prediction and class = 3 ==> 1, prediction = 3 and class = 2 ==> 0.5, prediction = 3 and class = 1 ==> 0.   So expectation is 0.5 and we would expect a score much greater than 0.5.  We filter to only consider prediction of 3. 


DO NOT CHANGE THIS PARAMETERS ANYMORE... THEY WILL ALLOW US TO COMPARE V2 with V6
This models we are dealing with 49 variables (including index and ret_5w), so we really have 47+1 predictor variables. 

Overall Feeling: 
AMD on V6.2 had a worst result than V2.2 (that's unexpected).. but also I then changed slightly the period of training and testing (I used 65% of data forfirst training), and that changed dramatically the outcome of my metrics (That's also unexpected and again show the fragility of my model)

Results for comparison: 

AMD/QQQ/SMH
Model v2.2 prec: 0.622, soft_prec: 0.774, recall: 0.278, soft_acc: 0.596. 14, 533, 24, 0.00175, 9
Model v6.2 prec: 0.461, soft_prec: 0.585, recall: 0.382, soft_acc: 0.679. 14, 601, 16, 0.00029, 6
Model v6.2.2prec: 0.672,soft_prec: 0.824, recall: 0.346, soft_acc: 0.598. 17, 427, 20, 0.000221, 8

TSM/QQQ/SMH
Model v2.2 prec: 0.481, soft_prec: 0.654, recall: 0.827, soft_acc: 0.655. 13, 792, 15, 0.00064, 7
Model v6.2 prec: 0.500, soft_prec: 0.642, recall: 0.483, soft_acc: 0.658. 15, 473, 20, 0.000161, 7

AXP/SPY/XLF
Model v2.2 prec: 0.517, soft_prec: 0.647, recall: 0.427, soft_acc: 0.606. 17, 595, 24, 0.00811, 10
Model v6.2 prec: 0.547, soft_prec: 0.624, recall: 0.305, soft_acc: 0.550. 13, 484, 27, 0.000560, 9

BAH/QQQ/HACK
Model v2.1 prec: 0.490, soft_prec: 0.661, recall: 0.732, soft_acc: 0.639.  12, 717,29, 
Model v6 prec: 0.505, soft_prec: 0.675, recall: 0.636, soft_acc: 0.708.  13, 422, 19,   ***

BBWI/SPY/XLY
Model v2.2 prec: 0.559, soft_prec: 0.686, recall: 0.366, soft_acc: 0.563. 13, 456, 16, 0.00120, 8
Model v6.2 prec: 0.289, soft_prec: 0.829, recall: 0.465, soft_acc: 0.585. 7, 778, 27, 0.00262, 9

FDX/SPY/IYT
Model v2.2 prec: 0.553, soft_prec: 0.691, recall: 0.510, soft_acc: 0.735. 17, 680, 23, 0.00807, 9
Model v6.2 prec: 0.323, soft_prec: 0.510, recall: 0.294, soft_acc: 0.567. 14, 437, 29, 0.00197, 7

GILD/SPY/XLV
Model v2.2 prec: 0.300, soft_prec: 0.527, recall: 0.297, soft_acc: 0.602. 13, 422, 23, 0.00113, 7
Model v6.2 prec: 0.304, soft_prec: 0.481, recall: 0.344, soft_acc: 0.475. 13, 793, 25, 0.00781, 6

PKI/SPY/XLV
Model v2.2 prec: 0.557, soft_prec: 0.682, recall: 0.443, soft_acc: 0.692. 12, 486, 16, 0.000671, 10
Model v6.2 prec: 0.354, soft_prec: 0.480, recall: 0.285, soft_acc: 0.625. 18, 722, 22, 0.00520, 10

SBUX/SPY/XLY
Model v2.2 prec: 0.481, soft_prec: 0.612, recall: 0.515, soft_acc: 0.658. 13, 699, 29, 0.00774
Model v6.2 prec: 0.308, soft_prec: 0.469, recall: 0.690, soft_acc: 0.585. 15, 542, 23, 0.00492,6

DIS/SPY/VOX
Model v6 prec: 0.484, soft_prec: 0.562, recall: 0.503, soft_acc: 0.648.  13, 772, 14, 0.00138

PARA/SPY/VOX
Model v2.2prec: 0.500, soft_prec: 0.561, recall: 0.381, soft_acc: 0.630. 18, 442, 20, 0.000292, 7
Model v6 prec:  0.519, soft_prec: 0.634, recall: 0.629, soft_acc: 0.694. 15, 630, 15, 0.00928
Model v6.2prec: 0.405, soft_prec: 0.548, recall: 0.227, soft_acc: 0.600. 15, 475, 16, 0.00250, 8

CAT/SPY/XLI



# Running the model 

```{r script_model}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 300)

library(dplyr)
library(rsample)
library(parsnip)
library(recipes)
library(tune)
library(workflows)
library(dials)
library(yardstick)

# setting up the path within the project for easier portability of code
thePath <- here::here()
source(paste0(thePath, "/models/functions/wrangle_data_v06.R"))

ticker <- "AMD"
etf1 <- "QQQ"
etf2 <- "SMH"


model_type = "5wClassThirdile"
print(paste0("Dealing now with ", ticker))
df0 <- conform_data(ticker, interval = "Daily", provider = "Tiingo") %>% 
  create_base_fin_df_v6() #%>% filter(Index <= today() - 100)  #taking this out because unecessary.  
                                                                # I will remove it when I fit the model. 
df1 <- relative_returns(ticker, etf1, etf2)
df <- left_join(df0, df1, by = "Index")
rm(df0, df1)

df_model <- reduce_df_v6(df) %>% na.omit() %>% mutate(ord_class = as.factor(ntile(ret_5w, 3))) 
rm(df)
print(paste0("The ", model_type, " build on ", ticker, " has ", nrow(df_model), " rows"))
  
skip <- 13
prediction_advance = 40 #(5 weeks ahead)
starting_percentage = 0.5 
based_sample = floor((nrow(df_model) - (10 * prediction_advance)) * starting_percentage) 
if (based_sample < 2500) { 
  while (based_sample < 2500 & starting_percentage < 0.8) { 
    starting_percentage = starting_percentage + 0.05
    based_sample = floor((nrow(df_model) - (10 * prediction_advance)) * starting_percentage) 
  } 
} 
skippy = floor((nrow(df_model) - prediction_advance - based_sample) / 12) - 1 
time_slices <- rolling_origin(df_model, initial = based_sample, assess = prediction_advance, 
                              lag = -skip, 
                              skip = skippy, cumulative = TRUE) %>% 
  mutate(train = map(splits, training), validation = map(splits, testing)) 
print(paste0("First sample has ", based_sample, " rows and training is skipping ", skippy, " rows."))
print(paste0("There are ", nrow(time_slices) ," time slices created."))  

recipe_base <- recipe(formula = ord_class ~., data = df_model) %>% 
  update_role(Index, new_role="ID") %>% step_rm("ret_5w") 
model_xgboost <- boost_tree(trees = tune(), mtry = tune(), 
                            loss_reduction = tune(), tree_depth = tune(), 
                            learn_rate = tune(), min_n = tune(), stop_iter = tune()) %>% 
set_engine("xgboost") %>% set_mode("classification") 

stopping_grid <-grid_latin_hypercube(trees(range = c(500, 800)), 
                                     loss_reduction(range = c(-10, 1.5)), 
                                     mtry(range = c(12L, 19L)), tree_depth(range=c(6L, 10L)), 
                                     learn_rate(range = c(-4, -1)), 
                                     min_n(range = c(15L, 30L)),  stop_iter(range = c(10L, 40L)), 
                                     size = 120) 
wf <- workflow() %>% add_recipe(recipe_base) %>% add_model(model_xgboost) 
```

Running the grid search 
```{r}
print("Starting now the resampling process")
doParallel::registerDoParallel()
xgboost_resampling <- tune_grid(wf, time_slices, 
                                grid = stopping_grid, 
                                metrics = metric_set(accuracy, mn_log_loss), 
                                control = control_resamples(save_pred = TRUE, save_workflow = TRUE))

print("resampling process over.  Looking now for best model.")
yo_soft_acc <- xgboost_resampling %>% collect_predictions() %>% 
  mutate(soft_acc = case_when(.pred_class=="3" & ord_class == "3" ~ 1, 
                            .pred_class!="3" & ord_class != "3" ~ 1, 
                            TRUE ~ 0)) %>% 
  group_by(.config) %>% 
  summarize(mean_soft_acc = sum(soft_acc)/n(), 
            mtry = mean(mtry), tree_depth = mean(tree_depth), 
            trees = mean(trees), 
            min_n = mean(min_n), learn_rate = mean(learn_rate))

# if I predict a 3, is it indeed a 3? Higher is better
yo_precision <- xgboost_resampling %>% collect_predictions() %>% 
  mutate(spec_3 = if_else(.pred_class == "3" & ord_class == "3", 1, 0)) %>% 
  filter(.pred_class == "3") %>% 
  group_by(.config) %>% 
  summarize(mean_preci = sum(spec_3)/n())

# if there is a "3" in the actual returns, do I manage to predict it! Higher is better
yo_recall <- xgboost_resampling %>% collect_predictions() %>% 
  mutate(sens_3 = if_else(.pred_class == "3" & ord_class == "3", 1, 0)) %>% 
  select(-mtry, -min_n, -learn_rate, -loss_reduction) %>% 
  filter(ord_class == "3") %>% 
  group_by(.config) %>% 
  summarize(mean_recall = sum(sens_3)/n())

# if I predict a 3, is it indeed a 3? Higher is better
yo_soft_preci <- xgboost_resampling %>% collect_predictions() %>% 
  mutate(spec_3 = case_when(.pred_class=="3" & ord_class == "3" ~ 1, 
                            .pred_class=="3" & ord_class == "2" ~ 0.5, 
                            .pred_class=="3" & ord_class == "1" ~ 0, 
                            TRUE ~ 0)) %>% 
  #select(-mtry, -min_n, -learn_rate, -loss_reduction) %>% 
  filter(.pred_class == "3") %>% 
  group_by(.config) %>% 
  summarize(mean_soft_preci = sum(spec_3)/n())

yo_cost3 <- left_join(yo_precision, yo_recall) %>% left_join(., yo_soft_preci) %>% 
  left_join(., yo_soft_acc) %>% 
  mutate(soft_acc_rank = percent_rank(mean_soft_acc), 
         recall_rank = percent_rank(mean_recall), 
         preci_rank = percent_rank(mean_preci), 
         soft_preci_rank = percent_rank(mean_soft_preci), 
         avg_rank =  0.5 * preci_rank + 0.2 * soft_preci_rank + 0.15 * soft_acc_rank + 0.15 * recall_rank)

yo_metric <- left_join(yo_precision, yo_soft_acc) %>% left_join(., yo_recall) %>% 
  left_join(., yo_soft_preci) %>% left_join(., yo_cost3) %>% 
  select(.config, avg_rank, mean_preci, mean_soft_preci, mean_soft_acc, mean_recall,  
         mtry, tree_depth, trees, min_n, learn_rate) %>% 
  arrange(desc(avg_rank), mean_preci)

rm(yo_sens, yo_spec, yo_cost3, yo_acc, yo_log, yo_cost_acc)

write_csv(yo_metric, paste0(thePath, "/models/models_metrics/xgboost_modelv62_", model_type, "_", ticker, ".csv"))

best_model <- yo_metric$.config[1]

best_param <-xgboost_resampling %>% collect_metrics() %>% filter(.config == best_model) %>% slice_tail()
best_param

final_model <- finalize_model(model_xgboost, best_param)
final_model

final_wf <- workflow() %>% add_recipe(recipe_base) %>% add_model(final_model)
final_wf

model_base <- fit(object = final_wf, data = df_model)
write_rds(model_base, paste0(thePath, "/models/models_raw/xgboost_modelv62_", model_type, "_", ticker, ".rda"))

library(vip)
df_prep <- prep(recipe_base)
var_importance <- final_model %>% 
  set_engine("xgboost", importance = "permutation") %>% 
  fit(ord_class ~ ., data = juice(df_prep) %>% select(-Index)) %>%
  vi()
write_csv(var_importance, paste0(thePath, "/models/var_importance/xgboost_modelv62_", model_type, "_", ticker, ".csv"))
rm(df_prep)

ntiles <- tibble("min"=1:3, "max"=1:3)
for (i in 1:3) { 
  ntiles$min[i] <- min(df_model %>% filter(ord_class == i) %>% select(ret_5w))  
  ntiles$max[i] <- max(df_model %>% filter(ord_class == i) %>% select(ret_5w))
}
ntiles


```


# Other useful functions for tidymodels  

## Download data
```{r download, message=FALSE, warning=FALSE}
library(tidyverse)
thePath <- here::here()
source("../functions/get_data.R")
get_tiingo_prices("XLF")
```



Check the models on newer data
```{r create_predictions, message=FALSE, warning=FALSE}
library(tidymodels)
thePath <- here::here()
source(paste0(thePath, "/models/functions/wrangle_data_v06.R"))
ticker <- "TSM"
etf1 <- "QQQ"
etf2 <- "SMH"
test0 <- conform_data(ticker, interval = "Daily", provider = "Tiingo") %>% 
  create_base_fin_df_v6() %>% mutate(ord_class = as.factor(ntile(ret_5w, 3))) 
test1 <- relative_returns(ticker, etf1, etf2)
test <- left_join(test0, test1, by = "Index") %>% 
  reduce_test_df_v6() %>% filter(Index > today() - 100)
rm(test0, test1)

# taking the long side V6
prediction1 <- tibble(Index = test$Index, 
                      prediction = predict(read_rds(paste0("models_raw/xgboost_modelv6_5wClassThirdile_", ticker, ".rda")), 
                                           new_data = test, type="prob")) %>% 
  mutate(prob1 = prediction$.pred_1, prob2 = prediction$.pred_2, prob3 = prediction$.pred_3) %>% 
  select(-prediction)

prediction2 <- tibble(Index = test$Index, 
                      prediction = predict(read_rds(paste0("models_raw/xgboost_modelv6_5wClassThirdile_", ticker, ".rda")), 
                                           new_data = test)) %>% 
  mutate(pred_ord_class = prediction$.pred_class) %>% select(-prediction)

yo_v6 <- left_join(test, prediction1) %>% 
  select(Index, ret_5w, ord_class, prob1, prob2, prob3) %>% 
  left_join(., prediction2) 


# taking the short side
source(paste0(thePath, "/models/functions/wrangle_data_v02.R"))
test0 <- conform_data(ticker, interval = "Daily", provider = "Tiingo") %>% 
  create_base_fin_df_v2() %>% mutate(ord_class = as.factor(ntile(ret_5w, 3))) 
test1 <- relative_returns(ticker, etf1, etf2)
test <- left_join(test0, test1, by = "Index") %>% 
  reduce_test_df_v2() %>% filter(Index > today() - 100)
rm(test0, test1)
prediction1 <- tibble(Index = test$Index, 
                      prediction = predict(read_rds(paste0("models_raw/xgboost_modelv2short_5wClassThirdile_", ticker, ".rda")), 
                                           new_data = test, type="prob")) %>% 
  mutate(prob1 = prediction$.pred_1, prob2 = prediction$.pred_2, prob3 = prediction$.pred_3) %>% 
  select(-prediction)

prediction2 <- tibble(Index = test$Index, 
                      prediction = predict(read_rds(paste0("models_raw/xgboost_modelv2short_5wClassThirdile_", ticker, ".rda")), 
                                           new_data = test)) %>% 
  mutate(pred_ord_class = prediction$.pred_class) %>% select(-prediction)
yo_v6_short <- left_join(test, prediction1) %>% 
  select(Index, ret_5w, ord_class, prob1, prob2, prob3) %>% 
  left_join(., prediction2)
```




```{r get_dates}
# for plotting
get_date <- function(x) {
  min(assessment(x)$Index)
}

start_date <- map(time_slices$splits, get_date)
time_slices$start_date <- do.call("c", start_date)
head(time_slices$start_date)
```



