---
title: "V08 - Exploration of for modeling"
author: "FdR"
date: "4-12-2023"
output: html
---

```{r setup}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

# TO DO 

* check if there are NA where there are NA.  Imputation have to be done before. 

```{r}
#| label: basics
#| message: false
#| warning: false

library(dplyr)
library(purrr)
library(readr)
library(lubridate)
library(tidyr)

the_path <- here::here()

### Function to harmonize the data. 
### ### This is copied from summary_report_v3
conform_data <- function(ticker){
    df <- read_csv(paste0(the_path, "/data_stock_fmpr/", ticker, ".csv"), 
                   show_col_types = FALSE) |>  
      select(date, open, high, low, close, adjusted=adjClose, volume) |>  
      arrange(date) |> 
      filter(date < today() - 252)
    return(df)
}

```

# Building the basic df 

## Ticker df 

```{r base_df}
library(roll)
library(timetk)

corr_roll_252 <- slidify(.f = ~cor(.x, .y), .align = 'right', .period = 252)
mean_roll_21 <- slidify(.f = mean, .align = 'right', .period = 21)
mean_roll_252 <- slidify(.f = mean, .align = 'right', .period = 252)
sd_roll_252 <- slidify(.f = sd, .align = 'right', .period = 252)

augment_df_fin <- function(df){
  df <- df |>   
    arrange(date) |>  
    mutate(roc21 = log(adjusted / lag(adjusted, n = 21)), 
           
           ema11 = TTR::EMA(adjusted, n = 11), 
           ema23 = TTR::EMA(adjusted, n = 23), 
           sma47 = TTR::SMA(adjusted, n = 47), 
           sma50 = TTR::SMA(adjusted, n = 50), 
           sma101 = TTR::SMA(adjusted, n = 101), 
           sma199 = TTR::SMA(adjusted, n = 199), 
           sma200 = TTR::SMA(adjusted, n = 200), 
           perc_abov_ema11 = log(adjusted / ema11), 
           perc_abov_ema23 = log(adjusted / ema23), 
           perc_abov_sma47 = log(adjusted / sma47), 
           perc_abov_sma101 = log(adjusted / sma101), 
           perc_abov_sma199 = log(adjusted / sma199), 
           sd_abov_ema11 = (ema11 - mean_roll_252(perc_abov_ema11)) / sd_roll_252(perc_abov_ema11), 
           sd_abov_ema23 = (ema23 - mean_roll_252(perc_abov_ema23)) / sd_roll_252(perc_abov_ema23), 
           sd_abov_sma47 = (sma47 - mean_roll_252(perc_abov_sma47)) / sd_roll_252(perc_abov_sma47), 
           sd_abov_sma101 = (sma101 - mean_roll_252(perc_abov_sma101)) / sd_roll_252(perc_abov_sma101),  
           sd_abov_sma199 = (sma199 - mean_roll_252(perc_abov_sma199)) / sd_roll_252(perc_abov_sma199),  
           corr_ema23_sma101_252d = corr_roll_252(ema23, sma101), 
           corr_ema23_sma47_252d = corr_roll_252(ema23, sma47), 
           corr_sma50_sma200_252d = corr_roll_252(sma50, sma200), 
           
           volum_sma200 = TTR::SMA(volume,  n = 200), 
           volum200_perc = log(volume / volum_sma200), 
           
           volat_roc21_252d = sd_roll_252(roc21), 
           volat_volu200_252d = sd_roll_252(volum200_perc), 
           
           ret_7w = (lead(adjusted, n = 35) / adjusted) - 1) 
  
  return(df)
  
}

reduce_df <- function(df) { 
  df <- df |> 
    select(date, 
           sd_abov_ema11, sd_abov_ema23, sd_abov_sma47, sd_abov_sma101, sd_abov_sma199, 
           corr_ema23_sma101_252d, corr_ema23_sma47_252d, corr_sma50_sma200_252d,  
           volat_roc21_252d, volat_volu200_252d, 
           ret_7w)
}

```

## modeling 

```{r}
ticker <- 'AMD'

## Modeling phase now
model_type = "7wClassThirdile"
df0 <- conform_data(ticker) |> augment_df_fin()
df_model <- reduce_df(df0) |> arrange(date) |> 
  mutate(ord_class = as.factor(ntile(ret_7w, 3))) %>% na.omit() 


print(paste0("Dealing now with ", ticker))
print(paste0("The ", model_type, " build on ", ticker, " has ", nrow(df_model), " rows"))
  

library(rsample)
min_size_training <- 500
number_of_samples <- 50
skippy <- 63
laggy <- 21
# get the size of the df we are using. 
validation_size <- floor(nrow(df_model) * 0.2 / number_of_samples) + 1
training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49

if (training_size < 500) {
  training_size <- min_size_training
  skippy <- floor((nrow(df_model) - validation_size - laggy - min_size_training) / number_of_samples) 
  training_size <- nrow(df_model) - (skippy * (number_of_samples - 1)) - (laggy + validation_size) - 49
}

time_slices <- rolling_origin(df_model, 
                              initial = training_size, assess = validation_size + laggy, 
                              lag = -laggy, skip = skippy, 
                              cumulative = FALSE)

library(recipes)
recipe_base <- recipe(formula = ord_class ~., data = df_model) |> 
  update_role(date, new_role="ID") |>
  step_rm("ret_7w") 

library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
model_xgboost <- boost_tree(trees = tune(), mtry = tune(), loss_reduction = tune(), 
                            tree_depth = tune(), 
                            learn_rate = tune(), min_n = tune()) %>% 
  set_engine("xgboost") |>  
  set_mode("classification") 

stopping_grid <-grid_latin_hypercube(trees(range = c(400, 800)), 
                                     loss_reduction(range = c(-10, 1.5)), 
                                     mtry(range = c(12L, 17L)), 
                                     learn_rate(range = c(-4, -2)), 
                                     min_n(range = c(17L, 29L)),   
                                     tree_depth(range = c(6L, 9L)), 
                                     size = 10) 
wf <- workflow() |> add_recipe(recipe_base) |> add_model(model_xgboost)  

print("Starting now the resampling process")
doParallel::registerDoParallel()
xgboost_resampling <- tune_grid(wf, time_slices, 
                                grid = stopping_grid, 
                                metrics = metric_set(accuracy, mn_log_loss), 
                                control = control_resamples(save_pred = TRUE, save_workflow = TRUE))

print("resampling process over. Looking now for best model.")

yo_soft_acc <- xgboost_resampling |> collect_predictions() |> 
  mutate(soft_acc = case_when(.pred_class=="3" & ord_class == "3" ~ 1, 
                              .pred_class!="3" & ord_class != "3" ~ 1, 
                              TRUE ~ 0)) |> 
  group_by(.config) |> 
  summarize(mean_soft_acc = sum(soft_acc)/n(), 
            mtry = mean(mtry), tree_depth = mean(tree_depth), 
            trees = mean(trees), 
            min_n = mean(min_n), learn_rate = mean(learn_rate))

# if I predict a 3, is it indeed a 3? Higher is better
yo_precision <- xgboost_resampling %>% collect_predictions() %>% 
  mutate(spec_3 = if_else(.pred_class == "3" & ord_class == "3", 1, 0)) %>% 
  filter(.pred_class == "3") %>% 
  group_by(.config) %>% 
  summarize(mean_preci = sum(spec_3)/n())

# if there is a "3" in the actual returns, do I manage to predict it! Higher is better
yo_recall <- xgboost_resampling %>% collect_predictions() |>  
  mutate(sens_3 = if_else(.pred_class == "3" & ord_class == "3", 1, 0)) |> 
  select(-mtry, -min_n, -learn_rate, -loss_reduction, -tree_depth) |> 
  filter(ord_class == "3") |> 
  group_by(.config) |> 
  summarize(mean_recall = sum(sens_3)/n())

# if I predict a 3, is it indeed a 3? Higher is better
yo_soft_preci <- xgboost_resampling |> collect_predictions() |> 
  mutate(spec_3 = case_when(.pred_class=="3" & ord_class == "3" ~ 1, 
                            .pred_class=="3" & ord_class == "2" ~ 0.5, 
                            .pred_class=="3" & ord_class == "1" ~ 0, 
                            TRUE ~ 0)) %>% 
  filter(.pred_class == "3") |>  
  group_by(.config) |> 
  summarize(mean_soft_preci = sum(spec_3)/n())

yo_cost3 <- left_join(yo_precision, yo_recall) %>% left_join(., yo_soft_preci) %>% 
  left_join(., yo_soft_acc) %>% 
  mutate(soft_acc_rank = percent_rank(mean_soft_acc), recall_rank = percent_rank(mean_recall), 
         preci_rank = percent_rank(mean_preci), soft_preci_rank = percent_rank(mean_soft_preci), 
         avg_rank =  0.5 * preci_rank + 0.2 * soft_preci_rank + 0.15 * soft_acc_rank + 0.15 * recall_rank)

yo_metric <- left_join(yo_precision, yo_soft_acc) |> left_join(yo_recall) |> 
  left_join(yo_soft_preci) |> left_join(yo_cost3) |> 
  select(.config, avg_rank, mean_preci, mean_soft_preci, mean_recall, mean_soft_acc, 
         mtry, tree_depth, trees, min_n, learn_rate) |> 
  arrange(desc(avg_rank), mean_preci)

#rm(yo_sens, yo_spec, yo_cost3, yo_acc, yo_log, yo_cost_acc)

write_csv(yo_metric, 
          paste0(the_path, "/models/model_metrics/xgboost_modelv8_explo3_", model_type, "_", ticker, ".csv"))

best_model <- yo_metric$.config[1]

best_param <-xgboost_resampling |> collect_metrics() |> 
  filter(.config == best_model) |> slice_tail()
best_param

final_model <- finalize_model(model_xgboost, best_param)
final_model

final_wf <- workflow() |> add_recipe(recipe_base) |> add_model(final_model)
final_wf

model_base <- fit(object = final_wf, data = df_model)
write_rds(model_base, 
          paste0(the_path, "/models/model_raw/xgboost_modelv8_explo3_", model_type, "_", ticker, ".rda"))

library(vip)
df_prep <- prep(recipe_base)
var_importance <- final_model %>% 
  set_engine("xgboost", importance = "permutation") %>% 
  fit(ord_class ~ ., data = juice(df_prep) %>% select(-date)) %>%
  vi()
write_csv(var_importance, 
          paste0(the_path, "/models/model_vars_imp/xgboost_modelv8_explo3_", model_type, "_", ticker, ".csv"))
rm(df_prep)


```



## picking up the important variables 

```{r}

```


volat_roc1_19days and volat_roc1_63days and volat_roc5_29days are all highly correlated. So, I will keep volat_roc5_29days and then use the correlation of volat_roc1_19days and volat_roc1_63days.  DONE


# Summary of important variables and correlation (5 weeks)

                          V2    V3    V6    V7       Corr
volat_volu200_252days     1            1             X
volat_roc21_252days       2            4    2        X
min_252                   3                          min_126
ma_cross_l                4            11            X
std_away_sma200_1         5     9                    X
volat_volu200_21days      7                          X
atr14 (melt_atr13)        8                          volat_roc1_19days, volat_roc5_29days, +1
max_252                   9                          max_63
adx_14 (melt_adx17)       10                     
volat_roc1_19days         11                         volat_roc1_63days, volat_roc5_29days
volat_roc5_29days         12                         volat_roc1_63days
min_63 (meltp_min_63)           13                   min_126              
max_63                    17                      
std_away_roc126_126       3*                   
corr251_ema20_sma50             1      3    4              
volat_roc1_63days               2      10   7        volat_roc1_19days / volat_roc5_29days
corr127_ema20_sma50             3      13                   
corr251_volum_sma47_200         4
atr19 (cor63_atr13_atr19)       5
corr252_rsi19_sma50             6                    corr126_rsi13_ema40
corr126_rsi13_ema40             8                    corr252_rsi19_sma50
corr199_sma50_sma200                   2

zzmark_volat251_roc61_ms               5
zzmark_volat251_roc23_ts               6
zzmark_volat251_roc61_tm               7
zzmark_volat251_roc23_ms               8

NEW
corr63_atr13_atr19, 







# Missing values 

## Checking on missing values or Zero values 

Checking for missing values and dealing with them. 
```{r mis_val, eval=FALSE}
check_for_na <- function(ticker) { 
  df <- conform_data(ticker, "Daily", "fmpr") %>% 
    select(index, open, high, low, close, adjusted, volume) %>% 
    summarize_all((~ sum(is.na(.))))
  return(df)
}

check_for_zero <- function(ticker) { 
    df <- conform_data(ticker, "Daily", "fmpr") %>% 
    select(open, high, low, close, adjusted, volume) %>% 
    filter(if_any(everything(.), ~ . == 0))
  }

## checking for 0 or NA
check_na <- tibble(tickers = c(tickers, etf1, etf2)) %>% 
  mutate(number_na = map(tickers, check_for_na)) %>% 
  unnest(cols = c(number_na))

check_naa <- tibble(tickers = c(tickers, etf1, etf2)) %>% 
  mutate(number_zero = map(tickers, check_for_zero)) 
df <- check_naa %>% unnest()
```


## fixing NA values 

NA values seems to appear mainly in the Volume part of the data. 
Noticing missing values in GILD, VOX and VAW.  You can have several days in a row with NA in volume.  

Solution: 
We are not touching the volume of the ETF.  So we are only interested in the volume of the stock. 
Noticing that when volume is NA, it is because they were no trading.  So we can just remove these days. 
```{r eval=FALSE}
the_path = here::here()
yo <- read_csv(paste0(the_path, "/data_stock_fmpr/GILD.csv"))
yo_na <- yo %>% filter(is.na(volume))
yo_no_na <- yo %>% filter(is.na(volume) == FALSE)

create_base_fin_df_v8 <- function(df){
  df <- df %>%  
    select(index, open, high, low, close, adjusted, volume) %>% arrange(index) %>% 
    mutate(open = as.numeric(open), high = as.numeric(high), low = as.numeric(low), 
           close = as.numeric(close), adjusted = as.numeric(adjusted), 
           volume = as.numeric(volume)) %>% filter(is.na(volume) == FALSE)
  
}
```



