{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"06-Exam3\"\n",
        "format: \n",
        "  pdf: \n",
        "    highlight-style: github\n",
        "    toc: true \n",
        "    toc-depth: 3\n",
        "    number-sections: true\n",
        "    colorlinks: true\n",
        "    papersize: A4 \n",
        "    fontsize: 11pt\n",
        "    include-in-header: \n",
        "      text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "         \\DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "execute: \n",
        "  warning: false\n",
        "  cache: true\n",
        "---"
      ],
      "id": "e92f44e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\newpage\n",
        "# Modeling Task \n",
        "\n",
        "Short-term asset return is a challenging quantity to predict.\n",
        "\n",
        "The objective of this task is to produce a model to predict positive moves (up trend) using a **Support Vector Machine** (SVM) model . The proposed solution should be \"comprehensive with detailed feature engineering and model architecture.\"  \n",
        "\n",
        "Before we start, let's remind ourselves of one of the main assumption in our quantitative finance journey: financial assets follow a Brownian motion; hence over a longer-ish period of time, we would expect our model to be predict half the time up and half the time down.  \n",
        "\n",
        "## SVM as a classifier.  \n",
        "\n",
        "**Support Vector Machine** is a machine learning algorithm that aims to find the hyperplane that maximizes the margins that separate either *'hardly'* or *'softly'* the data into different classes.  In this exercise, we are dealing with a binary classification (up or down).  \n",
        "The hyperplane can be modelled with \n",
        "$$\\theta^T \\cdot X + \\theta_0 = 0$$\n",
        "where \n",
        "\n",
        "* $\\theta$ is the parameters vector of the plane that maximize the margins that separate the output variables. $\\theta_T$ is the transpose of that vector. \n",
        "* $\\theta_0$ is a scalar vector.  How far is the hyperplane from the origin.  \n",
        "\n",
        "In the case of *hard-margin*, we want to maximize the margin width $$\\max_{\\theta, \\theta_0}\\frac{2}{|\\theta|}$$ such that there are no missclassifcation at all.  This might require to use non-linear hyperplane (*radial*, *polynomial* or *sigmoid*) or to go a higher dimension (*the Kernel Trick*) to ensure that data are then exactly separable. \n",
        "Which is the same as finding $$\\min_{\\theta, \\theta_0} \\sum_{i=1}^{N} \\frac{1}{2} |\\theta|^2$$\n",
        "\n",
        "In the case of *soft-margin* where we have to weight in (with regularization) the cost of missclassification against wider margins; the loss function becomes: \n",
        "$$J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N max\\left( 1-y^{(i)} \\cdot (\\theta^T X^{(i)} + \\theta_0), 0 \\right) + \\lambda|\\theta|^2$$\n",
        "\n",
        "* $N$ is the number of observation in our dataset  \n",
        "* $y^{(i)}$ is the binary target variable, the output\n",
        "* $X^{(i)}$ is the feature vector, the inputs\n",
        "* $max\\left( 1-y^{(i)} \\cdot (\\theta^T X^{(i)} + \\theta_0), 0 \\right)$ is the loss function\n",
        "* $\\lambda|\\theta|^2$ is the regularization term\n",
        "\n",
        "## Choice of ticker \n",
        "\n",
        "To execute the task, we will use a mining stock **Rio Tinto Group** with ticker 'RIO'.  \n",
        "\n",
        "As of today (22 May 2025), this is a screenshot of the stock on Yahoo Finance. \n",
        "![Screenshot of Rio](rio-screenshot.png)\n",
        "\n",
        "\n",
        "## Loading Python packages, data & Initial EDA \n",
        "\n",
        "We'll start by loading the various Python packages required for our ML task. \n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "91e661f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: loading-packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf \n",
        "import mplfinance as mpl\n",
        "\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
        "\n",
        "from sklearn.metrics import roc_curve, ConfusionMatrixDisplay, accuracy_score, auc\n",
        "from sklearn.metrics import classification_report"
      ],
      "id": "loading-packages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's retrieve the price of Rio Tinto and store it in a *Pandas* dataframe. \n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "242e6b2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: loading-data\n",
        "#| output: false\n",
        "\n",
        "# downloading the data and storing it in a dataframe\n",
        "rio = yf.download('RIO')\n",
        "df = pd.DataFrame(rio)"
      ],
      "id": "loading-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring the time-series\n",
        "\n",
        "A quick look into the first 5 rows to check the structure.  we'll also check to see if there are missing values. \n"
      ],
      "id": "3e31c4cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: initial-eda\n",
        "\n",
        "# get to know df variables\n",
        "print('The shape of dataframe is:', df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# checking for missing values. \n",
        "df.isnull().sum()"
      ],
      "id": "initial-eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No missing values.  That's an encouraging start. \n",
        "\n",
        "A quick descriptive stat check on last 5 years. \n"
      ],
      "id": "70e14a53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "df.loc['2019-01-01':].describe().round(2)"
      ],
      "id": "a9e6d5af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vizualising the time-series\n",
        "\n",
        "Initial line plot of RIO stock over the last 5 years\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "91d85742"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot01\n",
        "\n",
        "# Checking only over the last 5 years of data \n",
        "start_date = date.today() + relativedelta(days=int(np.round(-5*365)))\n",
        "end_date = date.today()\n",
        "\n",
        "# first vizualization \n",
        "plt.plot(df['Adj Close'].loc[start_date:end_date])\n",
        "plt.show()"
      ],
      "id": "plot01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using candlesticks on last 18 months of data  \n"
      ],
      "id": "92653f7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: candlestick-plot01\n",
        "#| echo: false\n",
        "\n",
        "df_last18months = df.loc[start_date:end_date]\n",
        "\n",
        "fig, axlist = mpl.plot(\n",
        "  df_last18months, \n",
        "  type = 'candle', \n",
        "  style='binance', \n",
        "  figratio = (15, 7), \n",
        "  returnfig = True\n",
        ")\n",
        "\n",
        "fig = fig.suptitle(\n",
        "  f\"$RIO stock between {start_date} and {end_date}\", \n",
        "  y=.95, \n",
        "  fontsize=15, \n",
        "  weight = 'semibold', \n",
        "  style = 'normal'\n",
        ")\n",
        "\n",
        "mpl.show()"
      ],
      "id": "candlestick-plot01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Features Engineering \n",
        "\n",
        "Before starting the feature engineering process, we need to clarify what is understood by **'predicting positive moves (up trend)'.** Since *\"making sense of the instructions'* is part of the assessment, we are making the following assumptions.   \n",
        "\n",
        "  * the \"moves\" are based on the *closing prices* \n",
        "  * we are only trading during official market hours.  Making that assumption has some implication for trading as we would only know the closing price at the close of the trading session and hence technically we won't be able to trade that same day based on that information.  We would assume we make the trade before the closing bell  \n",
        "  * question to answer: during the trading session, would tomorrow's closing price be higher than today's closing price? Both are unknown at the moment of the trade. \n",
        "\n",
        "With these assumptions in mind, we'll start the process of feature engineering, with the suggested features presented in the exam instructions.  \n",
        "\n",
        "* We will create our binary classifier (*positive move*) based on the future asset price returns which we define as the log returns of the adjusted closing price.  \n",
        "$$Ret_t = log \\left( \\frac{P_{t+1}}{P_t} \\right) = log(P_{t+1}) - log(P_t)$$\n",
        "* As suggested in the list of possible *'basic'* features, we can use intraday moves as signal. As prices change overtime considerably (from a minimum of adjusted close of 25.7 to 73.6), we intuitively feel, the indicator might be more relevant if we scale it as a ratio of the difference of the 2 prices over the closing price.  And to avoid look ahead biais, we define the 2 features as\n",
        "$${(O-C)}_t = \\frac{(O-C)_{t-1}}{C_{t-1}} \\approx log \\left( \\frac{O_{t-1}}{C_{t-1}} \\right)$$ \n",
        "$${(O-O)}_t = \\frac{(O_t - O_{t-1})}{O_t} \\approx log \\left( \\frac{O_t}{O_{t-1}} \\right)$$\n",
        "$${(O-prevC)}_t = \\frac{(O_t - prevC_{t-1})}{O_t} \\approx log \\left( \\frac{O_t}{C_{t-1}} \\right)$$\n",
        "and \n",
        "$$(H-L)_{t} = \\frac{(H-L)_{t-1}}{L_{t-1}} \\approx log \\left( \\frac{H_{t-1}}{L_{t-1}} \\right)$$\n",
        "\n",
        "* Next, we used the lagged returns: $Ret_{t-1}, Ret_{t-2}, etc.$.  Without much initial thought, we'll create 17 of these lagged returns. \n",
        "\n",
        "* SMA defined as $$\\frac{1}{N} \\sum_{i=1}^N P_{t-1}$$\n",
        "* EMA defined as $$EMA_t = EMA_{t-1} + \\alpha \\cdot [P_t - EMA_{t-1}]$$\n",
        "\n",
        "To do so we'll use the pandas already built-in functionalities.  We have checked in the documentation that they match the expected behavior for the exponential moving average [Pandas Doc on EMW](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html). \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "fbc3fdc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: feature-eng-1\n",
        "\n",
        "# getting returns. We shift it as it constitutes the base of what we have to predict. \n",
        "df['ret_1d_target'] = np.log(df['Adj Close'].shift(-1) / df['Adj Close'])\n",
        "df['ret_1d_shifted'] = (np.log(df['Adj Close']).diff(1)).shift(1)\n",
        "df.head()\n",
        "\n",
        "# creating variables based on intra day. \n",
        "df['o-c'] = np.log(df.Open/df.Close).shift(1)   # Same day Open-Close\n",
        "df['h-l'] = np.log(df.High/df.Low).shift(1)     # Same day High-Low\n",
        "df['o-o'] = np.log(df.Open/df.Open.shift(1))    # Open-Previous day Open\n",
        "df['o-prevC'] = np.log(df.Open/df.Close.shift(1)) # Open - Previous day Close\n",
        "\n",
        "newcol = {}     # avoid df fragmentation warnings\n",
        "\n",
        "# creating our lag variables\n",
        "for lags in range(1, 17, 1): \n",
        "  newcol['ret_1d_lag' + str(lags) + 'd'] = df.ret_1d_shifted.shift(lags)\n",
        "  newcol['o-c-lag' + str(lags) + 'd'] = df['o-c'].shift(lags)\n",
        "  newcol['o-o-lag' + str(lags) + 'd'] = df['o-o'].shift(lags)\n",
        "  newcol['h-l-lag' + str(lags) + 'd'] = df['h-l'].shift(lags)\n",
        "  newcol['o-prevC-lag' + str(lags) + 'd'] = df['o-prevC'].shift(lags)\n",
        "\n",
        "# Creating momentum variables\n",
        "for days in range(2, 61, 3): \n",
        "  newcol['ret_'+ str(days)] = df.ret_1d_shifted.rolling(days).sum()\n",
        "  newcol['sd_' + str(days)] = df.ret_1d_shifted.rolling(days).std()\n",
        "\n",
        "# creating SMA percentage\n",
        "for lags in range(2, 61, 4): \n",
        "  newcol['sma_'+str(lags)+'d']=df['Adj Close'].rolling(window=lags).mean()\n",
        "  newcol['sma_'+str(lags)+'d']=np.log(df['Adj Close']/newcol['sma_'+str(lags)+'d'])\n",
        "  newcol['sma_'+str(lags)+'d']=newcol['sma_'+str(lags)+'d'].shift(1)\n",
        "\n",
        "# creating EMA percentage\n",
        "for lags in range(3, 10, 2): \n",
        "  newcol['ema_'+str(lags)+'d']=df['Adj Close'].ewm(span=lags, adjust=False).mean() \n",
        "  newcol['ema_'+str(lags)+'d']=np.log(df['Adj Close']/newcol['ema_'+str(lags)+'d'])\n",
        "  newcol['ema_'+str(lags)+'d']=newcol['ema_'+str(lags)+'d'].shift(1)\n",
        "\n",
        "df_newcol = pd.DataFrame(newcol)\n",
        "df = pd.concat([df, df_newcol], axis = 1)\n",
        "\n",
        "print(df.shape)"
      ],
      "id": "feature-eng-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target Definition and model setup \n",
        "\n",
        "Our initial thought to define the target variable is to use the median return over the last 5 years.  We expect this to be a slightly positive value.  The main advantage of using the median is that it ensures we have an almost perfectly balanced dataset in regards to class.  \n",
        "\n",
        "In this sense, we define the label as \n",
        "$$y_i = \\begin{cases} 1 & \\text{if } x_i\\geq \\text{median(return)}\\\\ 0 & \\text{otherwise} \\end{cases} $$ \n",
        "\n",
        "\n",
        "### Final removing of NaN values and target definition \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "49d05ec9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: create-target\n",
        "\n",
        "# slicing the dataframe to get all data after 2019 (close to last 5 years)\n",
        "df_new = df.loc['2019-01-01':].copy()\n",
        "df_new.shape\n",
        "\n",
        "# Step 2: Calculate the median of the targetcolumn in the filtered data\n",
        "median_return = df_new['ret_1d_shifted'].dropna().median()\n",
        "print(f\"Median Return: {np.round(median_return, 6)}\")\n",
        "\n",
        "# Step 3: Create the 'target' column based on the median return\n",
        "df_new['target'] = np.where(df_new['ret_1d_target'] > median_return, 1, 0)\n",
        "df_new.shape\n",
        "df_new['target'].value_counts(normalize = True).round(4) * 100\n",
        "df_new.dropna(inplace = True)\n",
        "\n",
        "# Step 4: Remove all unwanted columns and target variables\n",
        "df3 = df_new.drop(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'target', 'ret_1d_target'], axis = 1).copy()\n",
        "\n",
        "# checking again size of data frame\n",
        "print(f\"Final data set contais {df3.shape[1]} features\")\n",
        "\n",
        "# setting up our inputs and output dataframe\n",
        "X = df3\n",
        "y = df_new['target']"
      ],
      "id": "create-target",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the data \n",
        "\n",
        "Creating the training and testing set. Note that the training set will be use for cross-validation as well.  Testing set will only be used twice: for our initial *base model* and once we have chosen our *final model*.  \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "bfbd5add"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training-testing\n",
        "\n",
        "# put shuffle to False as we are dealing with a timeseries where order of data matter.\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
        "\n",
        "print('Shape of x_train:', x_train.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n",
        "print('Shape of x_test:', x_test.shape)\n",
        "print('Shape of y_test:', y_test.shape)\n",
        "print(f\"Size of training set is {len(x_train)} and size of testing set is {len(x_test)}\")"
      ],
      "id": "training-testing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using 80-20 for training and testing leaves a bit over a year of data (272 trading days) for testing.  \n",
        "\n",
        "### Checking for correlations \n",
        "\n",
        "Although some ML algorithms are more robust to highly correlated variables (and SVM in theory is robust to correlated variables); it is generally good practice avoiding to bring in highly correlated variables. There are several reasons for this: \n",
        "\n",
        "* redundancy (we prefer parsimonious model, *simpler is better* principle), \n",
        "* model interpretability (when several features are strongly correlated it becomes difficult to know which one affect the output variable), \n",
        "* model generalization. When features are strongly correlated, model can perform well on training set but generalize poorly in testing set. \n",
        "* 'curse of dimensionality'. More variables than necessary degrade the model performances. \n",
        "* stability of feature importance. With highly correlated feature, a small change in the input data can lead to significant changes in the variable importances. \n",
        "\n",
        "It is expected that many of our features are highly correlated (especially with the SMA and EMA).  this is because there is auto-correlation in the prices.  There are $144 X 144$ possible correlation pairs or $10296$ pairs if we remove all the self-correlated and the duplicate ones.  \n",
        "\n",
        "Let's for instance look at the 10 most correlated (positive or negative) variables. \n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "5e18784c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: correlations\n",
        "\n",
        "corr_matrix = df3.corr()\n",
        "\n",
        "# unstack the get the list of all pairs and put it in dataframe\n",
        "corr_p = corr_matrix.unstack()      \n",
        "corr_p_df = pd.DataFrame(corr_p,columns=['Correlation']).reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "corr_p_df.columns = ['feature_1', 'feature_2', 'Correlation']\n",
        "\n",
        "# Remove self-correlations\n",
        "corr_p_df = corr_p_df[corr_p_df['feature_1'] != corr_p_df['feature_2']]\n",
        "\n",
        "# Drop duplicate pairs \n",
        "corr_p_df['abs_corr'] = corr_p_df['Correlation'].abs()\n",
        "corr_p_df = corr_p_df.sort_values(by='abs_corr', ascending=False).drop_duplicates(subset=['Correlation']).drop(columns=['abs_corr'])\n",
        "\n",
        "# Sort the pairs by the absolute value of the correlation\n",
        "sorted_corr_p = corr_p_df.sort_values(by='Correlation', ascending=False)\n",
        "\n",
        "# Select the top 20 pairs\n",
        "top_10_corr_pairs = sorted_corr_p.head(10)\n",
        "\n",
        "print(f\"There are {len(corr_p_df[corr_p_df['Correlation'].abs() > 0.8])} highly correlated pairs (above 0.8) from the {len(sorted_corr_p)} possible combination.\")"
      ],
      "id": "correlations",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is clearly opportunities to reduce our amount of variables and/or reduce the correlation of our features. \n"
      ],
      "id": "cb782030"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: correlations_table\n",
        "#| echo: false\n",
        "\n",
        "top_10_corr_pairs.iloc[0:9, ]"
      ],
      "id": "correlations_table",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have 2 options to build our base models.  Either, we transform the data through dimensionality reduction (PCA) to remove correlation or we select only a few 'less' correlated features.  In the next section, we will explore both cases. \n",
        "\n",
        "## Base model \n",
        "\n",
        "### Base model with PCA \n",
        "\n",
        "The intuition behind using PCA for our base model is 2 folds.  \n",
        "\n",
        "* remove all form of correlation between data.  This is because all the components are orthogonal to each other in the hyperspace.  \n",
        "* reduce the number of features (**parsimony principle**).  As our dataframe contains highly correlated features, we can reduce the number of features and still keep a high amount of variance in our data.  \n",
        "\n",
        "After some trials, we decided to keep 95% of the variance in our data through the use of PCA. \n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "6c7daa99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: num-pca-components\n",
        "\n",
        "#let's scale the data first \n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "print(x_train_scaled.shape)\n",
        "\n",
        "pca = PCA()      # apply PCA with all the components\n",
        "x_train_pca = pca.fit_transform(x_train_scaled)\n",
        "x_train_pca.shape\n",
        "\n",
        "cumul_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(cumul_var_ratio[1:30])\n",
        "\n",
        "# to keep 95% of the variance \n",
        "num_comp = np.argmax(cumul_var_ratio > 0.95) + 1\n",
        "print(f\"{num_comp} components already explain 95% of the variance\")"
      ],
      "id": "num-pca-components",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "44 components is a lot less than the initial 144 and we are not loosing that much information. \n",
        "\n",
        "Now the number of components to use for a base model is itself a parameters that we can tune.  Let's see what different number of components with different kernels provide us. \n",
        "To make sure that we are not fine tuning anything with the test set in mind and also knowing that with such high number of features, we will check results on the *cross-validation training set*  using *time-series split*. \n",
        "Using a pipeline, we can now create our first base model using PCA. \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "94b0ffc2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: base-model-pca\n",
        "#| warning: false\n",
        "\n",
        "pca_levels = [0.8, 0.85, 0.9, 0.95, 0.99]\n",
        "kernel_type = ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "df_xval_score = pd.DataFrame(columns = ['pca_levels','kernel_type','xval_score'])\n",
        "tscv = TimeSeriesSplit(n_splits = 10, gap = 1)\n",
        "\n",
        "\n",
        "for level in pca_levels: \n",
        "  for k_type in kernel_type: \n",
        "    model_svm_base = Pipeline([ \n",
        "      ('std_scaler', StandardScaler()),\n",
        "      ('minmax_scaler', MinMaxScaler()), \n",
        "      (\"pca\", PCA(n_components = level, random_state = 42)), \n",
        "      ('classifier', SVC(kernel=k_type, probability=True, random_state=42))\n",
        "      ]) \n",
        "    \n",
        "    score_xval = cross_val_score(estimator = model_svm_base, \n",
        "                                 X = x_train, y = y_train, cv = tscv, \n",
        "                                 scoring = 'roc_auc', \n",
        "                                 n_jobs = 3).mean()\n",
        "    new_row = pd.DataFrame({'pca_levels': [level], \n",
        "                            'kernel_type': [k_type], \n",
        "                            'xval_score': [score_xval]}) \n",
        "    df_xval_score = pd.concat([df_xval_score, new_row], ignore_index=True)\n",
        "    \n",
        "df_xval_score = df_xval_score.sort_values(by = 'xval_score', \n",
        "                                          ascending = False)"
      ],
      "id": "base-model-pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gt_table-scores-pca\n",
        "#| echo: false\n",
        "\n",
        "df_xval_score.iloc[0:5, ]"
      ],
      "id": "gt_table-scores-pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First interesting observation is that the *rbf* kernel has not made the top of the list. Considering the non-linear nature of financial time-series, I would have intuitively thought it would be a better kernel to use. \n",
        "\n",
        "So 80% of the variance seems enough to get a 'decent' base model. That's equivalent to "
      ],
      "id": "77291e55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_comp = np.argmax(cumul_var_ratio > 0.8) + 1\n",
        "print(f\"{num_comp} components explain 80% of the variance\")"
      ],
      "id": "149506be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From 144 features to 23 features.  That's a considerable reduction! \n",
        "\n",
        "#### Checking base model metrics \n",
        "\n",
        "As our model is well-balanced we can use accuracy as a metric. Not a massive difference between training set and testing set.  That's encouraging.  Results are not very encouraging just a bit over 50% accuracy.  That's expected is come from the assumption that financial assets behaved in a stochastic way (Brownian motion)\n",
        "\n",
        "We now fit our first base model with the best PCA level and kernel. \n",
        "\n",
        "NOTE: empircally, we have noticed that the *MinMaxScaler()* has more impact on the cross-validation score than the *StandardScaler()*.  Also the order seems to matter: *StandardScaler()* first then *MinMaxScaler()* \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "55e95e31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: base-model-pca-fit\n",
        "#| output: false\n",
        "\n",
        "model_svm_base = Pipeline([\n",
        "  ('std_scaler', StandardScaler()), \n",
        "  ('minmax_scaler', MinMaxScaler()), \n",
        "  (\"pca\", PCA(n_components=df_xval_score.iloc[0,0], random_state = 42)),\n",
        "  ('classifier', SVC(kernel=df_xval_score.iloc[0,1], \n",
        "                     probability=True, random_state = 42))\n",
        "])\n",
        "\n",
        "model_svm_base.fit(x_train, y_train)"
      ],
      "id": "base-model-pca-fit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have now a first look at the metrics.  \n"
      ],
      "id": "0bce7de9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: base-model-pca-metrics\n",
        "\n",
        "acc_train = accuracy_score(y_train, model_svm_base.predict(x_train))\n",
        "acc_test = accuracy_score(y_test, model_svm_base.predict(x_test))\n",
        "\n",
        "print(np.round(acc_train, 4))\n",
        "print(np.round(acc_test, 4))"
      ],
      "id": "base-model-pca-metrics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training accuracy is quite higher than testing set indicating that we might still have quite a bit of overfitting. \n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "Checking the confusion matrix. \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "7aebdd52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: confusion_matrix-basemodel-pca\n",
        "\n",
        "conf_mat = ConfusionMatrixDisplay.from_estimator(model_svm_base, x_train, y_train, cmap = plt.cm.Blues)\n",
        "plt.title('Confusion Matrix on Base Model')\n",
        "plt.show()"
      ],
      "id": "confusion_matrix-basemodel-pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's pretty informative.  The model fails mainly at identifying the up day it seems (aka poor recall).  This is maybe something we can live with (at this early stage, it seems like poor recall would be better than poor precision.)\n"
      ],
      "id": "6f50bf24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: classification_report_pca\n",
        "\n",
        "y_test_pred = model_svm_base.predict(x_test)\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "id": "classification_report_pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ROC - AUC report \n",
        "\n",
        "Accuracy is an appropriate metrics when we have a balanced dataset (as in our case).  The Receiver-Operator Curve (ROC) is another metrics relevant to classification task.  It measures the True Positive Rate (TPR) also called sensitivity or recall against the False Positive Rate (FPR) (1 - sensitivity).  What is of interest is the Area Under the Curve.  An area of 0.5 being random state, anything above 0.5 shows shows increase TPR and low FPR. \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "2a8763cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: roc_auc_pca\n",
        "\n",
        "y_prob = model_svm_base.predict_proba(x_test)[:, 1]\n",
        "\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(np.round(roc_auc, 4))\n",
        "\n",
        "plt.clf()\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "id": "roc_auc_pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's not very encouraging.  This is very close to random guessing. \n",
        "\n",
        "### Base model with features selection \n",
        "\n",
        "Another way to reduce the number of available features is to use feature reduction techniques.  \n",
        "\n",
        "There are a lots of different techniques for this.  For this report, we will demonstrate 2 ways to select features.  The first one involve the filter method and the second one involve the embedded method. \n",
        "\n",
        "#### The filter method \n",
        "\n",
        "This method is not dependent of the machine learning algorithms we are using. We are using statistical tests to determine the *k-* most valuable features. Reading at the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) for this method, we have the choice between 3 main statistical test: ANOVA, Chi-squared and Mutual Information.  Let's try all three on the 3 different kernels to see which one can make a good base model.  \n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "0de45e2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: feat-sel-kbest\n",
        "\n",
        "stat_test_type = [chi2, f_classif, mutual_info_classif]\n",
        "kbest = [10, 15, 20, 25, 30, 35]\n",
        "df_xval_score = pd.DataFrame(columns = ['stat_test', 'kbest','kernel_type','xval_score'])\n",
        "tscv = TimeSeriesSplit(n_splits = 10, gap = 1)\n",
        "\n",
        "feat_scaling = Pipeline([('std_scaler', StandardScaler()), ('minmax_scaler', MinMaxScaler())])\n",
        "x_train_scaled = feat_scaling.fit_transform(x_train)\n",
        "x_test_scaled = feat_scaling.fit(x_test)\n",
        "  \n",
        "for stt in stat_test_type: \n",
        "  for kb in kbest: \n",
        "    for k_type in kernel_type: \n",
        "      skbest = SelectKBest(score_func = stt, k = kb) \n",
        "      x_train_sel = skbest.fit_transform(x_train_scaled, y_train)\n",
        "      #skbest.fit(x_train_scaled, y_train) \n",
        "      #x_skb = x_train_scaled.iloc[:, skbest.get_support()]\n",
        "      mod_svc = SVC(kernel = k_type, probability = True) \n",
        "      score_skb = cross_val_score(mod_svc, x_train_sel, y_train, \n",
        "                                  cv=tscv, scoring='roc_auc', n_jobs = 3).mean()\n",
        "      new_row = pd.DataFrame({'stat_test': [stt.__name__], 'kbest': [kb], \n",
        "                               'kernel_type': [k_type], 'xval_score': [score_skb]})  \n",
        "      df_xval_score = pd.concat([df_xval_score, new_row], ignore_index = True)\n",
        "      \n",
        "df_xval_score = df_xval_score.sort_values(by = 'xval_score', ascending = False)"
      ],
      "id": "feat-sel-kbest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the top combination of kernel, k-features and stat test in cross-validation is: "
      ],
      "id": "94d97afc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gt_table-scores-kbest\n",
        "#| echo: false\n",
        "\n",
        "df_xval_score.iloc[0:5]"
      ],
      "id": "gt_table-scores-kbest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now fit our model with the chosen kernel and k-best features. Then we can check our classification report and ROC\n",
        "\n",
        "\\AddToHookNext{env/Highlighting/begin}{\\small}"
      ],
      "id": "6e2c8c28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fit_kbest_test\n",
        "\n",
        "skbest = SelectKBest(score_func = f_classif, k = df_xval_score.iloc[0,1]) \n",
        "skbest.fit(x_train_scaled, y_train)\n",
        "x_train_skb = x_train.iloc[:, skbest.get_support()]\n",
        "x_test_skb  = x_test.iloc[:, skbest.get_support()]\n",
        "\n",
        "model_svm_reduc = Pipeline([\n",
        "  ('scaler', MinMaxScaler()), \n",
        "  ('minmax_scaler', MinMaxScaler()), \n",
        "  ('svm',SVC(kernel = df_xval_score.iloc[0,2], probability = True))])\n",
        "\n",
        "mod_svc_kbest = model_svm_reduc.fit(x_train_skb, y_train)\n",
        "\n",
        "mod_svc_kbest.fit(x_train_sel, y_train)\n",
        "\n",
        "y_prob = mod_svc_kbest.predict_proba(x_test_skb)[:,1]\n",
        "\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\" ROC AUC score for selected model is: {np.round(roc_auc, 4)}\")"
      ],
      "id": "fit_kbest_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features engineering revisited \n",
        "\n",
        "Now that we have a base model with associated metrics (ROC and accuracy). We can try to be a bit more creative in getting features that we think would help us increase the *'relevance'* of our model.  \n",
        "\n",
        "As shown in the previous section, \n",
        "\n",
        "## Hyper-parameters tuning \n"
      ],
      "id": "4e481796"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: hyper-param\n",
        "#| eval: false\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 10, gap = 1)\n",
        "\n",
        "model_svm_tuned = Pipeline([\n",
        "  #('Scaler', MinMaxScaler()), \n",
        "  ('std_scaler', StandardScaler()), \n",
        "  (\"pca\", PCA(n_components = 0.95)),\n",
        "  ('clf', SVC(probability = True))\n",
        "])\n",
        "\n",
        "param_grid = {'clf__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20], \n",
        "              'clf__gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10, 20], \n",
        "              'clf__kernel': ['rbf', 'poly', 'linear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(model_svm_tuned, param_grid, verbose = 1, \n",
        "                    scoring = 'roc-auc', cv = tscv, n_jobs = 3)\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "grid.best_params_\n",
        "grid.best_score_\n",
        "#grid.cv_results_"
      ],
      "id": "hyper-param",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting tuned model \n"
      ],
      "id": "6bbe2cd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "svm_best_param = SVC(**grid.best_params_)\n",
        "svm_best_param.fit(x_train, y_train, \n",
        "                   #eval_set = [(x_train, y_train), (x_test, y_test)], \n",
        "                   #verbose = True\n",
        "                   )\n",
        "#x_val_score = cross_val_score(svm_best_param, x_train, y_train, cv = tscv)\n",
        "\n",
        "# what does this do?  \n",
        "x_train_tuned = svm_best_param.decision_function(x_train)\n",
        "x_val_score "
      ],
      "id": "0be7ff10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "y_pred = svm_best_param.predict(x_test)\n",
        "\n",
        "acc_tuned_train = accuracy_score(y_train, svm_best_param.predict(x_train))\n",
        "acc_tuned_test = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_test)"
      ],
      "id": "9b11c4c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Varibales importance \n"
      ],
      "id": "c483fe2f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}