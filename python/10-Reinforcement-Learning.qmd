---
title: "10 - Reinforcement Learning"
format: 
  html: 
      embed-resources: true
      toc: true
      number_section: true
---

# Introduction to Tensorflow

## Dealing with Tensors

Tensors are n-dimensional arrays 

```{python}
#| label: tensor01

import tensorflow as tf

# create a tensor.  Once created it keeps it shape
tensor = tf.constant([[list(range(3))], 
                      [list(range(1, 4))], 
                      [list(range(2, 5))]], dtype = tf.float32)

print(tensor)

# like in numpy, the '*' operator is for element-wise multiplication
print(tensor * tensor)

# for matric multiplication, use 'matmul'
print(tf.matmul(tensor, tensor))   ### error!!!!

# not sure why the 3x3 above doesn't work. 
ar = tf.constant([[1, 2], [2, 2]], dtype=tf.float32)
print(ar)
tf.matmul(ar, tf.transpose(ar))
tf.matmul(ar, ar)

## other way to do matrix multiplication 
tf.einsum('ij,jk->jk' , ar, ar)

# cross-product
tf.einsum('ij,kl->ijkl', ar, ar)

```

## slicing tensors 

```{python}
tensor = tf.constant([[list(range(3))], 
                      [list(range(1, 4))], 
                      [list(range(2, 5))]], dtype = tf.float32)
print(tensor)

tensor[1:, ]

tensor[:,:, 2]
```

## Use of variables

Remember that tensors are immutable after creation. 

* tf.Variable
* tf.convert_to_tensor
* tf.reshape

```{python}
tensor = tf.constant([[1, 2], [2, 4]])
print(tensor)

variable = tf.Variable(tensor)
print(variable)

# return the index of the max value in the tensor
print(tf.math.argmax(variable))

print(tf.convert_to_tensor(variable))

# change the variable tensors
print(variable.assign([[1, 2], [11, 13]]))

```

## Operations with matrices 

It comes from 2 additional library wihtin tf ... 
the *math* and *linalg* 


```{python}
import tensorflow as tf
ar = tf.constant([[2, 5], [7, -11]], dtype = tf.float32)

print(tf.math.abs(tensor))

print(tf.linalg.det(ar))
```

```{python}
import tensorflow as tf
import numpy as np

tensor = tf.constant(np.ones((3, 3), dtype = np.int32))
print(tensor)

print(tf.reduce_sum(tensor))
print(tf.reduce_sum(tensor, axis = 1))

# we can also define more complex functions 
@tf.function
def sigmoid_activation(inputs, weights, bias): 
  x = tf.matmul(inputs, weights) + bias
  return tf.divide(1.0, 1 + tf.exp(-x))

inputs = tf.constant(np.ones((1, 3), dtype = np.float64))
weights = tf.Variable(np.random.random((3, 1)))
bias = tf.constant(np.ones((1, 3), dtype = np.float64))
print(sigmoid_activation(inputs, weights, bias))
```

## Layers

Layers come from the *keras* library.  

```{python}
import tensorflow as tf
from tensorflow.keras.layers import Layer

class CustomDenseLayer(Layer): 
  def __init__(self, neurons): 
    super().__init__()
    self.neurons = neurons
    
  def build(self, input_shape): 
    self.wt = tf.Variable(tf.random.normal((input_shape[-1], self.neurons), dtype = tf.float32), trainable = True)
    self.bias = tf.Variable(tf.zeros((self.neurons, ), dtype = tf.float32), trainable = True)
    self.upperbound = tf.constant(0.9, dtype = tf.float32, shape = (input_shape[-1], ))
    
  def call(self, inputs): 
    return tf.matmul(tf.minimum(self.upperbound, inputs), self.wt) + self.bias
  
layer = CustomDenseLayer(5)
print(layer)
print(layer.weights)
print(layer.trainable_weights)

```

