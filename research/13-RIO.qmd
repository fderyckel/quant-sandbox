---
title: "03 - RIO"
format: html
---

Using Glmnet as feature selection, incrementally select appropriate variables to make a more complete models. 

As external source of data, we will use DIA, QQQ, Ratio DIA/QQQ YES!, Ratio RIO/SPY, Ratio RIO/USCI, Ratio RIO/XME, Ratio GDX/GLD YES!!, Ratio RIO/UUP YES!!

```{r}
#| label: setup
#| warning: false
#| message: false

library(readr)
library(dplyr)
library(glue)
library(timetk)
library(tidyr)

the_path <- here::here()

conform_data <- function(ticker) {
    df <- read_csv(paste0(the_path, "/data_stock_fmpr/", ticker, ".csv"), 
                   show_col_types = FALSE) |> 
      rename(adjusted = adjClose) |> arrange(date) |> 
      select(date, open, high, low, close, volume, adjusted) |> 
      mutate(ret_1d = log(adjusted / lag(adjusted, n=1)), 
             forw_ret = log(lead(adjusted, n = 41) / adjusted), 
             ord_class = ntile(forw_ret, 5), 
             target = case_when(ord_class == 1 ~ 1, 
                                ord_class == 2 ~ 1, 
                                ord_class == 3 ~ 2, 
                                ord_class == 4 ~ 3, 
                                ord_class == 5 ~ 3), 
             target = as.factor(target)) |> arrange(date)
  return(df)
}

primes <- c(5, 11, 19, 29, 37, 47, 59, 71, 83, 97, 
            109, 127, 157, 179, 199, 223, 251)

get_returns_sd <- function(df) {
  for (i in primes) { 
    ret_name = glue('ret_', i, 'd')
    sd_name = glue('sd_', i, 'd') 
    roll_sd <- slidify(.f = sd, .period = i, .align = 'right') 
    df <- df |> mutate(!!as.name(ret_name) := log(adjusted / lag(adjusted, i)), 
                       !!as.name(sd_name) := roll_sd(ret_1d)) 
  }
  return(df)
}

get_corr <- function(df) {
  for (i in primes) { 
    corr_name = glue('corr_', i, 'd')
    roll_corr <- slidify(.f = ~cor(.x, .y), .period = i, .align = 'right') 
    df <- df |> mutate(!!as.name(corr_name) := roll_corr(adj_tick, adj_etf))
  }
  return(df)
}


create_relative_prices <- function(df_tick, df_etf) {
  df_tick <- conform_data(df_tick) |> select(date, adj_tick = adjusted)
  df_etf <- conform_data(df_etf) |> select(date, adj_etf = adjusted)
  df <- left_join(df_tick, df_etf, by = c('date')) |> 
    mutate(adjusted = adj_tick / adj_etf, 
           ret_1d = log(adjusted / lag(adjusted))) |> 
    drop_na()
  return(df)
}

```

# Features engineering 

```{r}

df_ticker <- conform_data('XOM') 
df1a <- get_returns_sd(df_ticker) |> 
  select(-open, -high, -low, -close, -adjusted, -volume, 
         -ret_1d, -forw_ret, -ord_class) |> 
  drop_na()

tick1 <- 'VDE'
tick2 <- 'USCI'
df_rel <- create_relative_prices(tick1, tick2) |> 
  get_corr() |> 
  get_returns_sd() |> 
  drop_na() |> 
  select(-adj_tick, -adj_etf, -adjusted, -ret_1d) |> 
  rename_with(~tolower(glue("{tick1}_{tick2}_{.}")), -date)
#df1 <- df_rel
df1 <- left_join(df1, df_rel, by = c('date'))

yo <- df_ticker |> select(date, target)
df1 <- left_join(df1, yo, by = join_by(date))

### train-test split
#df_train = df1a[1:round(nrow(df1a)*0.8), ]
#df_test = df1a[(round(nrow(df1a)*0.8) + 1):nrow(df1a) , ]

df_rel_train = df1[1:round(nrow(df1)*0.8), ]
df_rel_test = df1[(round(nrow(df1)*0.8) + 1):nrow(df1) , ]

```

# (base) model 

```{r}
#| label: base_model_preprocessing
#| warning: false
#| message: false

library(recipes)
library(rsample)
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
library(ggplot2)

#rec_glmnet <- recipe(formula = target ~., data = df_train) |> 
#  update_role(date, new_role = 'ID') |> 
#  step_normalize(all_numeric_predictors())
#rec_xgboost <- recipe(formula = target ~., data = df_train) |> 
#  update_role(date, new_role = 'ID')
rec_xgboost_rel <- recipe(formula = target ~., data = df_rel_train) |> 
  update_role(date, new_role = 'ID')

#df_prep <- rec_glmnet |> prep(training = df_train)
#df_juiced_glmnet <- juice(df_prep)
#df_baked_glmnet <- df_prep |> bake(new_data = df_test)

#df_prep <- rec_xgboost |> prep(training = df_train)
#df_juiced_xgboost <- juice(df_prep)
#df_baked_xgboost <- df_prep |> bake(new_data = df_test)

df_prep_rel <- rec_xgboost_rel |> prep(training = df_rel_train)
df_juiced_xgboost_rel <- juice(df_prep_rel)
df_baked_xgboost_rel <- df_prep_rel |> bake(new_data = df_rel_test)

# create the CV validation resamples 
#tscv = rolling_origin(df_train, initial = 1250, assess = 100, lag = 20, skip = 350, cumulative = T)
tscv_rel = rolling_origin(df_rel_train, initial = 1250, assess = 100, lag = 20, skip = 220, cumulative = T)

#model_glmnet <- logistic_reg(penalty = tune(), mixture = tune()) |> 
#  set_engine('glmnet') |> set_mode(mode = 'classification')
# pure lasso for variable selections
#model_glmnet <- logistic_reg(penalty = tune(), mixture = 1) |> 
#  set_engine('glmnet') |> set_mode(mode = 'classification')
model_xgboost <- boost_tree(mtry = tune(), min_n = tune(), 
                            learn_rate = tune(), loss_reduction = tune(), 
                            tree_depth = tune(), trees = tune()) |> 
  set_engine('xgboost') |> 
  set_mode('classification')

# create wf
#wf_glmnet <- workflow(preprocessor = rec_glmnet, spec = model_glmnet)
#wf_xgboost <- workflow(preprocessor = rec_xgboost, spec = model_xgboost)
wf_xgboost_rel <- workflow(preprocessor = rec_xgboost_rel, spec = model_xgboost)

#glmnet_param <- parameters(penalty(), mixture())
#glmnet_grid_maxEnthropy = grid_max_entropy(glmnet_param, size = 200)

xgboost_param <- parameters(trees(range = c(500, 900)), 
                            loss_reduction(range = c(-5, -1)), 
                            mtry(range = c(23L, 38L)), 
                            learn_rate(range = c(-4, -2)), 
                            min_n(range = c(19L, 29L)),   
                            tree_depth(range = c(19L, 41L)))
xgboost_grid_maxEnthropy = grid_max_entropy(xgboost_param, size = 19)


rm(df_ticker, df1, yo, primes)
```

```{r}
#| label: glmnet

library(doParallel)
registerDoParallel()
glmnet_tuning <- tune_grid(wf_glmnet, resamples = tscv, 
                           grid = glmnet_grid_maxEnthropy, 
                           metrics = metric_set(f_meas, precision, accuracy, roc_auc), 
                           control = control_resamples(verbose = T, save_pred = T))

metrics_glmnet <- glmnet_tuning |> collect_metrics() |> 
  arrange(.metric, desc(mean))

best_glmnet_model <- glmnet_tuning |> show_best(metric = 'roc_auc', n = 1)

glmnet_tuning |> autoplot(metric = c('f_meas', 'roc_auc'))
```

```{r}
#| label: xgboost_single

library(doParallel)
registerDoParallel(cores = 3)
xgboost_tuning <- tune_grid(wf_xgboost, resamples = tscv, 
                            grid = xgboost_grid_maxEnthropy, 
                            metrics = metric_set(f_meas, precision, accuracy, roc_auc),
                            control = control_resamples(verbose = T, save_pred = T))

metrics_xgboost <- xgboost_tuning |> collect_metrics() |> 
  arrange(.metric, desc(mean))

best_xgboost_model <- xgboost_tuning |> show_best(metric = 'f_meas', n = 1)

xgboost_tuning |> autoplot(metric = c('f_meas', 'roc_auc'))
```

```{r}
#| label: xgboost_rel

library(doParallel)
registerDoParallel(cores = 3)
xgboost_tuning_rel <- tune_grid(wf_xgboost_rel, resamples = tscv_rel, 
                            grid = xgboost_grid_maxEnthropy, 
                            metrics = metric_set(f_meas, precision, accuracy, roc_auc),
                            control = control_resamples(verbose = T, save_pred = T))

metrics_xgboost_re <- xgboost_tuning_rel |> collect_metrics() |> 
  arrange(.metric, desc(mean))

best_xgboost_rel_model <- xgboost_tuning_rel |> show_best(metric = 'accuracy', n = 1)

xgboost_tuning_rel |> autoplot(metric = c('f_meas', 'roc_auc'))
```


# final metrics on test set 

```{r}

final_glmnet <- finalize_workflow(wf_glmnet, 
                                  parameters =  best_glmnet_model)

glmnet_model_tuned <- model_glmnet |> finalize_model(best_glmnet_model)
glmnet_final_fit <- glmnet_model_tuned |> fit(formula = target ~., data = df_juiced)
glmnet_pred_train <- predict(glmnet_final_fit, new_data = df_juiced)
glmnet_pred <- predict(glmnet_final_fit, new_data = df_baked)

df_pred <- bind_cols(df_baked |> select(date, target), glmnet_pred)
```

ARGHHHHH all same prediction ....

```{r}
final_xgboost <- finalize_workflow(wf_xgboost, 
                                  parameters =  best_xgboost_model)
xgboost_model_tuned <- model_xgboost |> finalize_model(best_xgboost_model)
xgboost_final_fit <- xgboost_model_tuned |> fit(formula = target ~., data = df_juiced_xgboost)
xgboost_pred_train <- predict(xgboost_final_fit, new_data = df_juiced_xgboost)
xgboost_pred <- predict(xgboost_final_fit, new_data = df_baked_xgboost)

df_pred_xgboost <- bind_cols(df_baked_xgboost |> select(date, target), xgboost_pred)
```

```{r}
final_xgboost_rel <- finalize_workflow(wf_xgboost_rel, 
                                  parameters =  best_xgboost_rel_model)
xgboost_model_tuned <- model_xgboost |> finalize_model(best_xgboost_rel_model)
xgboost_final_fit <- xgboost_model_tuned |> fit(formula = target ~., data = df_juiced_xgboost_rel)

xgboost_pred_train <- predict(xgboost_final_fit, new_data = df_juiced_xgboost_rel)
xgboost_pred <- predict(xgboost_final_fit, new_data = df_baked_xgboost_rel)

df_pred_xgboost_rel <- bind_cols(df_baked_xgboost_rel |> select(date, target), 
                                 xgboost_pred)

yo <- conf_mat(df_pred_xgboost_rel, truth = target, estimate = .pred_class)
summary(yo)
```


# variables importance

```{r}
library(vip)
library(forcats)

final_glmnet |> fit(df_juiced) |> 
  pull_workflow_fit() |> 
  vi(lambda = metrics_glmnet$penalty) |> 
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)) |>
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

```{r}
library(vip)
library(forcats)

yo <- final_xgboost |> fit(df_juiced_xgboost) |> 
  pull_workflow_fit() |> 
  vi() |> 
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)) 
write_csv(yo, file = 'temp/cvx_etf_xgboost_var.csv')

yo |> 
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

```{r}
library(vip)
library(forcats)

yo <- final_xgboost_rel |> fit(df_juiced_xgboost_rel) |> 
  extract_fit_parsnip() |> 
  vi() |> 
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)) 
write_csv(yo, file = 'temp/XOM_etf2_xgboost_var.csv') 

yo |> 
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```





# Final Model

```{r}
#| warning: false
#| message: false
library(stringr)
names_col <- read_csv('temp/rio_ret_sd_xgboost_var.csv') |> 
  slice_head(n = 15) |>  pull(Variable) 
df_ticker <- conform_data('RIO') |> get_returns_sd() |> 
  drop_na() |> select(date, target, matches(names_col)) |> 
  rename_with(~str_c(names_col, '_tic'), names_col)

names_col <- read_csv('temp/rio_xme_xgboost_var.csv') |> 
  slice_head(n = 15) |> pull(Variable) 
df_xme <- create_relative_prices('RIO', 'XME') |> 
  get_corr() |> get_returns_sd() |> drop_na() |> 
  select(date, matches(names_col)) |> 
  rename_with(~str_c(names_col, '_xme'), names_col)

names_col <- read_csv('temp/rio_uup_xgboost_var.csv') |> 
  slice_head(n = 15) |> pull(Variable) 
df_uup <- create_relative_prices('RIO', 'UUP') |> 
  get_corr() |> get_returns_sd() |> drop_na() |> 
  select(date, matches(names_col)) |> 
  rename_with(~str_c(names_col, '_uup'), names_col)

names_col <- read_csv('temp/gld_gdx_xgboost_var.csv') |> 
  slice_head(n = 15) |> pull(Variable) 
df_gld <- create_relative_prices('GLD', 'GDX') |> 
  get_corr() |> get_returns_sd() |> drop_na() |> 
  select(date, matches(names_col)) |> 
  rename_with(~str_c(names_col, '_gld_gdx'), names_col)

names_col <- read_csv('temp/gld_gdx_xgboost_var.csv') |> 
  slice_head(n = 15) |> pull(Variable) 
df_dia <- create_relative_prices('DIA', 'QQQ') |> 
  get_corr() |> get_returns_sd() |> drop_na() |> 
  select(date, matches(names_col)) |> 
  rename_with(~str_c(names_col, '_dia_qqq'), names_col)

df <- left_join(df_ticker, df_xme) |> left_join(df_gld) |> 
  left_join(df_uup) |> left_join(df_dia) |> drop_na() |> arrange(date)

### train-test split
df_train = df[1:round(nrow(df)*0.8), ]
df_test = df[(round(nrow(df)*0.8) + 1):nrow(df) , ]
rm(df_dia, df_gld, df_ticker, df_uup, df_xme)

library(recipes)
library(rsample)
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
library(ggplot2)

rec_xgboost <- recipe(formula = target ~., data = df_train) |> 
  update_role(date, new_role = 'ID')

df_prep <- rec_xgboost |> prep(training = df_train)
df_juiced_xgboost <- juice(df_prep)
df_baked_xgboost <- df_prep |> bake(new_data = df_test)

# create the CV validation resamples 
tscv = rolling_origin(df_train, initial = 1250, assess = 100, lag = 20, skip = 200, cumulative = T)

model_xgboost <- boost_tree(mtry = tune(), min_n = tune(), 
                            learn_rate = tune(), loss_reduction = tune(), 
                            tree_depth = tune(), trees = tune()) |> 
  set_engine('xgboost') |> 
  set_mode('classification')

wf_xgboost <- workflow(preprocessor = rec_xgboost, spec = model_xgboost)

xgboost_param <- parameters(trees(range = c(500, 900)), 
                            loss_reduction(range = c(-5, -1)), 
                            mtry(range = c(13L, 23L)), 
                            learn_rate(range = c(-4, -2)), 
                            min_n(range = c(19L, 29L)),   
                            tree_depth(range = c(11L, 19L)))
xgboost_grid_maxEnthropy = grid_max_entropy(xgboost_param, size = 40)
```

```{r}
library(doParallel)
registerDoParallel()
xgboost_tuning <- tune_grid(wf_xgboost, resamples = tscv, 
                            grid = xgboost_grid_maxEnthropy, 
                            metrics = metric_set(f_meas, precision, accuracy, roc_auc),
                            control = control_resamples(verbose = T, save_pred = T))

metrics_xgboost <- xgboost_tuning |> collect_metrics() |> 
  arrange(.metric, desc(mean))

best_xgboost_model <- xgboost_tuning |> show_best(metric = 'accuracy', n = 1)

#xgboost_tuning |> autoplot(metric = c('f_meas', 'roc_auc'))


```

And a resamples accuracy of 0.57. 
